{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdcd9ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "945a56a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "elc = pd.read_csv('datasets/ETTm2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f5bf765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f17a2ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69680, 8)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47254723",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x127564c70>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAD4CAYAAAAeugY9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAA/rklEQVR4nO2deXwV5dXHfyc7SSAhECCsYQmg7BARRBRZFERFq7ZqVWyt1NYqbrVYfd9X61LtYrVqa60bdd/FiqgIiKgoBGTfd4IsYQk7ZHveP+5MMnfurHfWe3O+fPjce+fOnTmZeebMmfOchYQQYBiGYRKblKAFYBiGYZzDypxhGCYJYGXOMAyTBLAyZxiGSQJYmTMMwyQBaX7urGXLlqK4uNjPXTIMwyQ8ixYt2iuEKDRax1dlXlxcjLKyMj93yTAMk/AQ0VazddjNwjAMkwRYUuZElE9E7xDRGiJaTURDiaiAiGYS0XrptbnXwjIMwzDaWLXMnwDwiRCiJ4B+AFYDmAJglhCiBMAs6TPDMAwTAKbKnIiaATgLwPMAIISoEkJUApgAYKq02lQAF3sjIsMwDGOGFcu8C4AKAC8S0fdE9BwR5QBoLYTYCQDSaysP5WQYhmEMsKLM0wAMBPBPIcQAAEdhw6VCRJOIqIyIyioqKuIUk2EYhjHCijIvB1AuhPhO+vwOIsp9NxEVAYD0ukfrx0KIZ4UQpUKI0sJCwzBJhmEYJk5MlbkQYheA7UTUQ1o0CsAqAB8CmCgtmwhgmicSMkwI+WLtHmzffyxoMRimHqtJQzcDeJWIMgBsAvAzRG4EbxHR9QC2AbjcGxEZJnxc9+JCZKalYO2D44IWhWEAWFTmQoglAEo1vhrlqjQOWbhlPy5/Zj7e/dUZGNSJw94ZbzlZUxe0CAxTT1JlgM5dG5lgfXTGmoAlYRiG8ZekUuZPzdkAAFiwZT827z0KbonHuMlbZdtRPGU6+8qZUJJUylzJOX/5Av+etyloMZgk4k+fRJ74hv9pTsCSMEwsSavMAWDx1sqgRWAYhvGFpFbmDMMwjQVf65mHgRPVtdhYcQST31iCzLQUTL9leNAiMQnCkZM1QYvAMLo0OmV+x9tLMX3ZzqDFYBKIBz5ahee/2oy0FApaFIbRJWncLOUHYiMMSOPaW7z1QNTn+z5ciTGPzfVKLCYJeP6rzQAAjo1iwkzSKPPKY9Vx/e6lb7Zg/Z4jmLNWs7QMw9RTW6evzu94aymmvLvMR2kYJpqkUeZ7Dp+wtJ7eBbmi/KCb4jCNjHcXl+ONhduDFoNpxCSNMv/5S+aNor9cV4E9h0/6IA3DMIy/JI0yt8LsNfquFC3/OsNYYcOeI0GLwDCNS5kbwZn/TLws2V4ZtAgMk9zKfMaKXagzmLRiGDfghzomDCS1MgciCt0KrPKZeGEXHRMGkl6Zn6iuDVoEJsl5fcG2oEVgmORX5latJjaumHhZuOWA+UoM4zFJr8yVLCuv1P2O3SyMFly7nEkUkl6Zy5b5qh8OYfG2ykBlYRKPW9743rVtFU+Zjvv/u9K17TGMkuRX5iBc/9JC3PjKoqBFYRIQt6OhXvx6i6vbYxiZRlE1cZZBshDDGFFrkoDArQmZsJD0yvyzVdZCExlGi7o64++Nim/JbN57FDsPHndJIobRJumV+cfLLcaZs4HFaFBnMjDMLHcg0o+WYbwm6X3mTOPi4LFqrNjhXgXM9SZ1V9SW+1JO7U8qjlfVYvPeo4628cmKXfhw6Q8uSaSPJWVORFuIaDkRLSGiMmlZARHNJKL10mtzb0X1Fs7iSw5+8ux8XPDkV65tz8yNUqPS5hOe/hrVtXWoqxN4ZMYay6WZmXAy6eUyy09Wm/ceRfGU6SieMj3KrXbjK4twy+vuRUXpYccyP0cI0V8IUSp9ngJglhCiBMAs6bPn7D9ahYPH42tEYQS7WZKDNbsOAwA+W+l8rmT1zkOm62jp+joh8O2mfXhm7kZM+g9HUSUy89bvBWBtolv5VLZWGod+4sTNMgHAVOn9VAAXO5bGAgMfmIl+93+GRVsPYP7GfX7skklA/vHFRke/rzxWhXFPzDNdTyt0UYgGXzpXVEwOrESoBv10b1WZCwCfEdEiIpokLWsthNgJANJrK60fEtEkIiojorKKigrnEktc+s9vcOW/v3VtezsPHseCzfuxaOt+zvpjcPhEjaX1rEyAMomP3RDUIEaF1WiWYUKIH4ioFYCZRLTG6g6EEM8CeBYASktLQzvy31i4Part15ZHxgcoTeNECIENe46gpHXToEWxjFm0C5Mc2D7LAQwLS5a5EOIH6XUPgPcBDAawm4iKAEB65cwcxhFvLyrHmL99iS/XufcEZ5c/f7rGVjSMXhw6cem2pCIR7tmmypyIcoioqfwewLkAVgD4EMBEabWJAKZ5JaQRlceqsHWfs9AhJhyslJToxgrnbdjiufZq6wSenrMRE57+2vJv3v9+R8yyqlqTTCMmYZD94HafwEQAprkVN0trAO9T5K9KA/CaEOITIloI4C0iuh7ANgCXeyemPqMfm4u9R6pc364QAhT0jEYjIyzHu7ZOWPaZv/TN5phl9324Ej8a0N5tsZgAsKPDleO38lg1fqg8jrb5TTyQShtTZS6E2ASgn8byfQBGeSGUHbxQ5EBk9jo1HLql0RHvI63TOinK3z84fZXF38QuW7z1AC4dyMo8mbA7tG5/aymA6Lm3BZv3Y3DnAjfFioIzQHXgAkrBYXbkf/7SQvS979OY5a98u1WxEfvnT/mLo1Xxd6gSAHZUci2WZMKK26S6xti9VrZ1v1viaMLKXAf51NXVCcxdV8HK3Qfkp1SzYz17zR4c0nCDyAkeQOT83f7WEizYbP0CitqtxfOtt9Zd7yyzvF8m/BgNh5raOnzw/Q7c8fZS/wTSgJW5DvKEx/NfbcbEFxbg05W7o75/c+G2QKMukhG7ESBHT9ag/MAxbNl7FEu2V+JoVYOCX1Z+EO8t3oFrnv/OdDvFU6bjtjeXxDVppXWR830/+dCbAJ2/cR+63TMDt765xHQbXkc4JX3VxHgRAuh+7wxUSY9O6hobv3t3OQCORw+S0gc/x3GXGna///0O3Dq6xJVtxcuy8kos3noA1w3rHKgcTCx692c3ExedwsrcgCoTHxjjHjNX7cYLX8dGhhhhRZGftHEOlcWQrBrX6kJbTrjoqUhIJCvz8OHG05bXwVrsZtHBalzpCZcsw8bODf8pq3//Q2XDU5AQ/s1Z2FH8MpXHYou+VR7zJsKKCZAEcJ2xMtdhiar5sxDA9GU7Ua1KCNm6j+u4uI3SQn+rbDsmvrAA7ywq92Rff5yxuv69W6n5WpOzduDJ9vARRBKQXViZ63DVc9ETZ5+v3o2bXluMJ2dvCEiixkn5gUiIn9Jad5N/zd1U/16pQ4PUp1EhlkxgKCtiuuJmcb4JQ1iZW2T/0cij866Dx/H9tgMBS8N4QVRkYoCW2IwV3LfWKUu2V+JvM9c52saz8xpu9B8siS3bEDZYmcfB56t3m6/EuIqTyaO/fLoWxVOmm64XlHtj/9EqlB+w5q77Yu0eFE+ZzmWaTbj46a/xxKz1jraxfndDjaD7/2stI9iIjgXZjrdhBEezxAFXxEssnprT4Bpbu+swlpZX4selHQKUKJrTHvrctD2dzNvS3MH32yvRwWPl0Nj5frvzJ/ANexo6DnVrlet4e0awMo8DZTuxRJgYSSYOn3DWMvC8x78EEMnQvHf8KVHfKc/kih3mLePcQq3Ief4zHGyqcF6NdfRjX9a/59DEEDJrDZdu9xtZwbkZ+//g9NXRCwJQovE2fOZnw/hZu+swtgUQheZ1VVC2zJmEwkt9G4RBfMWzsRmEVq55Nt7jR346c5q9/c3GveYrKUjxWJmzZc6EGjkUtKEIl3f7CmICdMve2Ed5tRhjH/8S/f/wGYBIrgNjjJ2ntxPVtZi9ZndcyX9X/du87o8Sr5+m2DK3iRt+NMY+soJzMkdhpqz9VuUHjlZZ6vq+Ztdh85WYepSJYGZNZia9vAhfrqvA6FNa47mJpZ7K5bVlzsrcIvJ5KNsaPcPNk1XeoKd4jzjIrjQ7V36fywc+0g53C0nDpYSk/MAxrFRMXgthfDzlyqfKcOO6OoHDJ51l8Wrh9XllZW4RVtr+0vf+z9A8OyNmuZOUe7Nf+h2ZpNcrlMdafBw9WYMzH50TtUwA2LDnCDZWHMF5vdoY/n7G8p0Y16cIj366JiozWGbfkZP4ofIE+rTPi0s+jmZJAMq27OfOMi5z+EQNtrmcGGPWMMLFAoiWCEvP02ThmEZ3KCEERj82F798eVHMd+r5ivmb9gGApiIHgAuf/AoXPvVV3PKxmyUkGJ2Hy56Zj9QUwsaHz/dPIMY27y42LtblVqEtP+BiXLHY1ZXqEspmh/SHg87qA3E0SxzceHZXNM109z6ld6Ll5VYz+JjwstPGxdqphfPsS7NL+40F2/DJCo5esYpm1ycb63vtZmOfuU26FuZgyrieePU7dyvPsSEUDsJyHvxoVjDlveXOd9LIMTpP6iexV77dhqK8Jp7Jwj5zm4zv2xYA0Kpppqvb1RsTifRozrhHiQt1Nthj7i5aytKutf3nT9e6JE0soXGzEFEqEX1PRB9JnwuIaCYRrZdem3snpg0k5frqL4a4vFlW2mEgLGehV7v4IhoAoM99n2JjxREc8SD8jYkmTJdtmOqZTwagLGYxBcAsIUQJgFnS58CRz12bvCxf9seWub+E4XC/fsMQnFrUNO7fHz5Rg1F/nYvPV9ur8cMGhTF2laXf167X0UuWlDkRtQcwHsBzisUTAEyV3k8FcLGrksXJ2N7GsaRNs+KbJjCbAGX8Ieib5ye3DsfQri0wtneRZ/vQcw18ud5eLRAm+vo8cLRK97tkwKpl/jiAuwAoI3FbCyF2AoD02krrh0Q0iYjKiKisoqLCiaymXDm4I3q1NX78nX3HiLi2vXa3dkp1ko2H0PL0nA0QQmDXIW/ax1mhW6tc9GzTLLD9HzrurPxvsqNl+dYokgeUaf5A/NeuEAJPzwlf+0hTZU5EFwDYI4SIjbq3gBDiWSFEqRCitLCwMJ5NWOZ3Y3uYrlPo8sRo0JZiY6Gqtg4vf7sVP3txYSD7f+/XZ+DF606LWvajge082ZeVIdWYh92Bo1X448erUaPKoNVyQymX1AlgliJtP1631XPzNsc1UdosTq+AVaxY5sMAXEREWwC8AWAkEb0CYDcRFQGA9Bp4ke+mWem+77MxX1R+81VAboZnrh6IgR2bx3T2+enpnTzZn5UhdbSq8U6ePvDRKvzry034bJV5+0b19blhT0MruHgv3Yc+Xm2+kgZpqd4GD5puXQhxtxCivRCiGMAVAGYLIa4G8CGAidJqEwFM80rIj5fvxF8s3AmDCfVSdvBmze4lVi5eL9Dzj6eleDPi9MaR0otwz/srPNl3InBSssjViXqaE4yKVdSHtaY2ua5XJ7eKRwCMIaL1AMZInz1h/sZ9mPrNFlzyj6+92kXcKMfTS99sCUwOxn9SPVLmnExsDfVh+mpD7JOb2g2q1Pf/+CJ8fm8n2FLmQogvhBAXSO/3CSFGCSFKpNf93ogYOQGHT9bg+22VputZYf7dI3FasTth8cqxMmPFLle2yYSHcQbRUV4lgXBpCGM2KlwlSia/8X3MMvWRVF6vuxzWWgkbCZEBavWisRrHWZTXBC1z3ZkIVd75+SJMPoZ1a6n7ndIy/0lpB9f2yePIGLlZh9odpeWdUlvmCzY32JzJ5hVNCGVuhbUPjrW1frVL/jLlgKjhizDpMFKsyvmsJhmpru1TL0KKuACAbZSHUkBENWP3u3691ySEMrdimWem2buY9BoD2CXaMve5IHaCUjxlOq55vqF/Ypgnjo1laxiXbk6GsmXuHkbnL8TDLi4SomqiU9fkjWd3xbjebVCpSLqottH01QilMk+22XEvmbd+LzbsOYyZq/bgl2d1CVocXYxOaWZagy3kpvuccxdi2XfkJApyMnRdqcc1GlMAxuGHyXacE8Qyd/b7wyeq0a9DPs7u3pC0VNwyBwDQv0O+o20rjSilRVVbJ/DLl8uwaKv2vHBNbR1+MbUMS7ZXOtp/InPZM/Px6CdrcKLGfmd0L7m4f9v690aWndJn/u95m13b/7rd2hN8jZXt+49h0IOf4xlVByDlqel3/2eav62L9rNEsTHJmrMnhDJ3WqAmXSNYX67RYhStYIUoN4vifcXhk/h05W786pXFmr/bfuA4Pl+9G7dqzMA3FmRrKmwGkt4NWo1XoYl6WLkMNlYcwS9fLsPJkN0gnSC3ZJyzRjsv8WRNraV+qiEbZq6TIMrc2e9H9PCujIDQGS1WZU72ASZTVydwsqbWNOU6DOjdoNUo53K8Su1XYuWmd8/7y/Hpyt1YtPWA5/KEBWWEihrlIXv/+x3eCxMgiaHMHc7ij+ihWQPMFZRznpv2HsUtr1uztOW/aOs+d5sWh5Xfv78cPe79JCEsJaWMdRYtc7MCb35zvKoWD01fhRPVyWOhx0OYJ9fdJiGUuRdPswM7RpKGejtoMgDEWm4fLv3B0faSlTcWbtf9LmwXXPvmDa3DjIKelMo81QePi9HTXlVNHbrcPR3fbopYqddPLcO/523Gy/PdbZ8YBHp/9q1vLjGN/AnZ0PKUhFDmXiTaje3dBgt+P8owKcQKYVNEYUfraIXtCP56RLf690ZuFqUyT/HBf260h4PHqzXLACRT7oNWXLjZ3EBjujwTQpl7lTbdqpnzbkRm10ojGktxI0IUnp+RmoK87Ibqm4ZuFsW4dLuLzM6Dx2OWGe3ijreXai4XEKiurUO1S3kVYYOkf3okW/ihEQmhzMOc96afrWeM1526w4rWk0yoLjjVeTlo0BAiRXH1uG2YD/3jbI2l+jv5cp1+45cBf5iJQQ/MdEGqxKMx9VlNDGWuofneuXFoAJLEkkRPsb6gdbgOnwjPBaceaeP76reHU1rma3dpd6Jyk3huGEJEFNqhEB1ju8jXvxCRuQE73NKIQn8TNgO0tLjA9Hez7zgbx3Qyw9zCzGdecfikpe0cPlGNf8/bjFtGdvO8iH2Q/H3W+phld76j7SIIArVLb0iXFrrrKn3mfrR003PlTFuS3CF3yj/789X2atpvCkli0N3jenq+j4TQGvH6zLsU5jqOVjFDazb95flbbG/n0U/W4O+z1mP68p0uSBVenpwdW0N6x4FY/3BQ2BlqSuXqxwOanmiT31jiw97Dgdp2ShR3ZUdVlyovSAhl7uf5GtnTXky6lpvlf6atNP2detLmeFXk8dHuY2RCIx07LyKC4s3O9GKy/ZSi4JpAK9mv6k7P+IcfN/uEUOZ+hH3J2M3kM4p2MOKbjcH0swwjXgz0W0aWWF73tV+cXv/ei5F2gYHf3Q5Oj9P8jftckSMoBGIt8TDNnRvhxyR/QihzPe4a2wMAMLBjvmvbtHvM73p3mebyf3yx0fB3U95brr1/e7tPCrwY5+f2am15XWUsuReGg1vWfmPNaUgQT4ohfpy6hFDm6ouhR+umAIBT2kQeX88sca/2ilvH/NXv7GXeJYrvz03k4kheWC12XBvKhyujUEQjjBS2W/PZ8RylP6saoW+qOKIZw54ICCFiFLuACN218+SVA2KWsZtFQn2ypv1mGADgnJ6t8NLPTsMtI7tp/Mo6lw1qX/9eCIEuUnlcJ8QdstgIja+g/+R4XWUA8MFNw7DwntG+WI9O73kCAiP/Olcnhp1xiwv7tcVHN58ZtcyPp6qEUObqJ9+s9IauQiN6tHIcyjf6lOhJTzcetZXW5rEq8xhfeY/J1srKCkG7DzLT4x8//Tvko7BpdD9ZdbNw95IvG47TGV31QyaTmUTxmZ9UBTKwz1zC+96H0dv/2bBix1tUnjv1o66mBJIIL3+buIWRpi/bieIp07F4m73yq0EmXk3o3xZDDWLJ4+HhS/pEfc7NbDA+mitKBdglquJkSJWYFxi5UcJ6GNTlE/zoKJkYytxjXa7cvhBAb5fLmW7eG524YBR+uGLHIVf37QbHq2px/hPzdLsmAcCBo1W46bVII46Pl9mLlQ8qnf8vl/fDE1cMcKeuimIT6s01axJR4KWdmtvuVesmeoc5UcrkRsQPmYNch+OqYxoKnzkRZRHRAiJaSkQrieh+aXkBEc0kovXSa3OzbcWL20WMYraveC8gXOm0rhQ5Ky0V97y/HFskpf7Xz8wt9TDx+Kx1WLXzEC7953zddY4qXEl2T5dXunyCov2bmvsv6hU1V+Iu0QdAnhwlctadyIsnmCXbK9Hzfz7BLJuZlUGx+9CJqM9Bu+j0UM/DhMVnfhLASCFEPwD9AYwloiEApgCYJYQoATBL+uyNkJ5b5toXn6NtKt4v2nYAr363rb5ORLkq4/Hg8eoY6z1MvKWoRf7NBvP4+LnrKmxFTDiZgDRCrlmvhZcXV5u86Gqc8nhy+mfWKJ7V45lb0RrWi6WORPPWJ0bew/99aJ6QFwbUmeEnfEgGNFXmIoLcYTZd+i8ATAAwVVo+FcDFXggIeP9gpd6+270dD5+IhLvp6Y8f/eNrLNxi7mfeVHEkrlIBTlhWXokDxxrC9a567jvT36zbfQQX/P0ry/tw282y8J7RAIAnZ8fWgfGK/CYZAIBLBrRDbmYatjwyvn5iVB5OTm8gyuMUjw82pEasLmt3HUbxlOlYsv2g7jph/ZPUobF+5D1a8pkTUSoRLQGwB8BMIcR3AFoLIXYCgPSqmQdPRJOIqIyIyioq9Mt0Ggrp8ZFQ+8xTXbDMlTfmE9WqK0+1eatdwi9++mv8z7SVnlmyWlgtIfr9tsqoz/tspI67/dfIw0UdUeDlPmW0agHJw8npaVPOqdX4MaMWMF+sjTRwniHVK0qkm1GHgmxseGhc/efzejlrHG8FS8pcCFErhOgPoD2AwUTU2+oOhBDPCiFKhRClhYXxJfd4bpmrlHmKhaPyy7O7eCeQDmEuY3qzxd6nWrhtmctujZsN8g+U4a2AcalbK1wztBPaNMvSTN3v2aYZOrXI1qyct+y+cy3vQ+kvzsm0X/BU6xyFLeFGiSyb0egQIpgp0dM7m1dtVbprW+ZmGqzpDrZGhBCikoi+ADAWwG4iKhJC7CSiIkSsdk/wfgI0uvqdFTeL9+GS+oTBQPnt20vx9qJynN29EHMNGiPsqDyOQpOB7PaDhnwRGTVZvlw1+ZmpylUoyMmwVZiqc8scfPv7UZrfZWekYu5vz4lZfu/4U9Asy3qoojLE1e1rIqwTiZYI4FJ88qoBGPzQLMN1/KwpBViLZikkonzpfRMAowGsAfAhgInSahMBTPNIRu+tB8X2WzfLtORmCcKiIZd8r27w9qJyADBU5EdP1mDYI7Px+/e169DIuP33yE9WRudInWimThyac8cIfDNlpKtyqWmenRH3b79cV4HHXIiKCrFhXm8wLdpqMJ8kvDesHrw41hHhVStLJ1hxsxQBmENEywAsRMRn/hGARwCMIaL1AMZIn70R0uMD112q9XLpwPY4s1vL+jtq08w0dCnUTu2PRyK3sjuDV+XWkBuDyL5PPaprvXGz2LnIbx/TI+pzXnY62uY3cVUuNWmpzsb13zVqwycTMdmeFtdzm6uHdIpZlpDKXAixTAgxQAjRVwjRWwjxB2n5PiHEKCFEifSqn1HiEOVhG9TJ/XD2dvlNsOaBsfjrj/uBiOot8zohMGPycFw7NPZkXudClqhdgjDIg3QnxYsyrluLARpVNlvkxG8l63Hl4I4AgNwsbW+mG1FT63d7364uzAjEFt/yA71TN76PO+WO4yHhMkB7tGnqyT6UE2KyZS4AZKal4j/zY1PsWzXN8iXcCIjEYT//1eb6z2qlvnrnIcxclRhJH16gTp+Xx4ue9fTABI3HZg9O5m2jS7D+oXHIztBW5mnSPiePKolbsbvVcCKMT3uPzFgTvUDDmhHCnxhuNco5i55tmqKd9BR353k99H7iOQmizBsO3IAO+Z7vT76wzKIs3I5H1+OzVbvwwEer6j+r3TXjnpiHG/5T5ossYUR9GvQs826tcrHlkfGetxKUISKkGxSBS5Wc+7eN6Y7Xbxjii0xqlM2Sw0aNhZlxAWDiCwu8F0aFcsx9cNOw+vfpDl1nTkgMZa54710KdgMNbhbj9ZTft3PRv/rJip1R9VvUTanDeOEFiUCkeqGMfKGpLyuze2+3Vrm4Y0x3N0XTJSM1JcplKPvPh3VrgYcusRz564iqmrr6TMVaFwfVviMnMe6Jedi+/5hr2wS8fXro1irX1vrKpz51mGtQJIQyV44zr8MUI/uQXqXPevXN5QthVM9WuHJwB9PtWr1ebnxlMf72+boYeWTufHspKo8F18/xeJXdwkzen7MMhQUsPzGpx0q/9vmG2/j89rNx8yjr7ebiQRZp1h1no0Dhpx/QIR+3ji7B337c31bz309Xxu9e637vDPxBeuJ77bttpuvX1NZh5qrdptFH05b8gNU7D0W5Bt1A60k5qMiuzLTwqc7wSaTBM3ONW7C5TWZaCiad1QXv3HgGAOBWHWstV0rcICLDED0ld7y1FNMtVBV8WfLTb99/DPuPRne/+WjZTs0u916gde/cWHEkdmGACAGkpzUIShpuljcnDcEDGiFmYYGIcOvo7mjVLCumroeaNMUjxgtfu6swjXhy9gbc8J8yfLE2vkxuI2au2m3anUsr6dUtVa53U5ioEfwAxIa26nHnud3xqqLHrJfYTyMLgAv6FvkahkVE+P35pzR81llPvqaWllei4vBJS9t+d3G5pfWOnKzBih0HccGT1mucNGa0fNPKR+HBnQt8eapzg3iyO/1g+4GI22TvEWtj3Q7ynM9PT9dWnoC24vbaMP+NjcbgXvzeDglhmXew8djpJ/LjvFVFbpc9h0/ofufX06U77hzvhdUKCVOq7kRR5ABwWrFxqnhQUyZ+hqlqPb166VLR27K6ixQQGz0FAM9cPQgX9WuLojxvcxOMSAhlfka3loHuXy/EzW7igN2xGIaJzhtfWRz3b/3SnwJCc2I8zPrbybkNOgPYj73LjU6i9quxY7cS8TJstJ5U7lM2Ivq0z8PfrxzgW4SbFgmhzJtJSReXDGgXyP7VSuHbuyM1OOx2aLFbUMppzZLt+49ZuvAPn6h2LV5ZyWKjNGwXEULb8pZvtm5GGjnFjRuMV0Uzq2vr8N7ict0xY1V2r26imtePS8fidxpF0JRF04pUNeoBYMND4/DklQPcEcAFEkKZN81Kx+w7zsYffxT7eBMEraRHr6M2ozrW7LKXrWekiM0skjW7DmH4n+bg3/M2me5nyMOzMPCBmZblsnqxJnI/U68Iw9OWHv+auxG3v7UU05b8YLxiQH+D1sSwW6LkNYkteHZYUaV02m8aYsnlc5iWmuJ7MS0jEkKZA0CXwtzA4jnlQXRKUTO8fsMQzRPYVaeGy7y7YqvlWcXJQN22LzJZ9fDHa+qbY+gh35TMoijsIg/6vUe8DaM0q9mxo9J61yO/CKMLSJ77OaAzTxK0yJs0unF5eXPMVrSPbNW0wTIP6/04YZR5kMgNeVvmZmBo19hO7md3L8RVOrPwTiZvnQxUpdvhawut3gBYjpyxKlc8N4ePbj7T8rpyp/uTOu6uMBZDCjPqbNBXvt2KpdsrY9az6qe26tsPeg4A0L5R6Y3frjp5J0HDytwCxS0iCnmYzkTsT07rgJpa9+tDGHX5MRv/ysFp9VpZvfOQpfWs+v7jaTphJ9X+t+f1RL8O+bh0oHZWcBh1+eM/6Y8zu7XU9MGGBQHgm417ce8HKzDh6a/rl8vH88Hpq13d3zYHmaJuTYBqzblkqxq7j+3VBs2z0wMPyNAjnAGtIaNTixx89/tRuk0WvLIAjSZYX/pmC+67qJel7bht91g1uHcd0g+tjIdurXKxYU9DwlL75k0wTVEX47lrS9FVkZa9+5A3IaNOKC0uwCsWk0iy0lNiWw76xPPz9JORDp+owRdr92BED81OkbZx4t2bq5PAlNckHQePG7sXlWhdwfdfFJ1k9sw1g+yI5jtsmVukdbMs3cmOFDJWmHeNja+SmpMwJ+X9xe22bFbdJ1v3uVubQx1Lrr6Jjj61NTorHoG9iv/3gyX/O6a+MbUXaOUPzDGpO6/kuhcX6n5nd9Q6MYWmvKfd+OR8m6VoteyxvGzrXaDCACtzF0hNIZxVot/f9Ncj9HtRGuHMZ+7OdrSwmsXqNhmqehilxca17cPgi42X/OwMNLXRUs4uWmWd5SfBRD5uMtUeuD3DDitzF0hJIZzatlnM8sd+3M/T/W7Yox/qqO5r6iZWijJ5gTKxo1fbZqbRTUmgk7Dg96Mw584Rrm83HmvYqwxQL7yUdprYPHP1wIRswqKGlbkL6PUMHdDRWVcks8md0Y99aW07klY7UV2LG19ehK37YkO8EoERPRqefqwoALfdS0HQqllWlOtIKx46Hv46c535Sg4J6uhveWQ8OrUwjiIbXtIwiTmkS4tQTpbbhZW5C+hNVCrHx3VnFNverhXftG4UjYab5ZuNe/HJyl34vw9XGm7zyMkaLCuvtCilf7Rv3nCBWrGkvMqUDBI7k3p2Ud77tA7dd5v3ebJfL6xis21e2Let5vJebZthXO82rsvjB6zMXcCK0l2zy1rYn5IaC42O9TrfR4UmSpemPMDNDNYbX16Ei5762na5Aq8hAp61EVFwmolPPRFpmet+r1I1eud9i8UJbSfdi9wqK2FqaZPyLdVPpmekpeAvl/dzlOwXFKzMXUCvS0vrZg2xxHZT+QFrN4l3F++of188ZTqmLdkRs069eNIANqu9/v22SE0VK2273MTMhZBChDY24rNLWjfFHyb0qq+lkwzo9RN1k798Zt0F8/HynTEZxikW2y7KKBXvwAdmYudB5xm7NnQ5AOCUoqa4ZWQ3PH3VQORkpoW2UqsRHGfuArLS7VqYg40VR/HNlJFoqyrupOdXN9yuhYtBrfAnv7EEE/q3i0qCsF2t0d7qrvGljjX082GdMaJHITLSUuqVubIIkhHXDi12S7xAyc1Mw5GTNVGNKbxErYi1rPX1uw/j168uxrjebfDPqxuemGQR452z2HXQeX6CWcnjnYp91AkBIsLt5wbXjNkN2DJ3AXnQvnPjGfjgpmExijxe8l2a7FIZ5lFUGXQ293tOSK8ZbkFOOs7qHpn8bNU0C6v/MBaTzurip2iBI1fn27T3KB70oWPSHlXC1X+XxhbfknvTqmvf1PfQjTM60I0HQju2UzJMlAMWlDkRdSCiOUS0mohWEtFkaXkBEc0kovXSa/I5KC0i+7ab52RENRZW0iTDfpGw8gPxP24qDTijwbpZo3iRTFiHeJOM1IRqNuEGR6saSjuo4+3dwuh8//adZfq/U/1Q9j/H2yT6pW+2xPU7JWaj4wxFjaVkmSi3MipqANwhhDgFwBAANxHRqQCmAJglhCgBMEv63Cgpad3UdB07j8fyDeGjZSalSA1IjXKzREbr32ett/TboNSkPEE7oGM+gIYa0kliODlCGWPvVTNh5XHebqFeit79VPaZW625ot7OJhd6zJoNmZ5FDXkhjcYyF0LsFEIslt4fBrAaQDsAEwBMlVabCuBij2QMLSvvPw8f3zJc1xpXYqf2OdX7HOMUDIgqPfC/0yKhiIu3VdYvu/2tJaitE5pRNscl/+hUFyykeGiRE6mB00zKgEyOS80d2uU3wdGTDWOpl0ayWrwoe3seNijyZobc8HvB5v34IaDyw2bGU25mGi7sFwlPzEoLprS229i6xRNRMYABAL4D0FoIsROIKHwAmlV3iGgSEZURUVlFhftdvYMkJzNNM/NTi442ZsflYeikIpyyrstJDb/4e4t34KnZGzD5jSUx38k3kT9/ujbu/ZtxRtcWGNlTu1DThf0ik5s9iyJPPEliODmiWnLldWuVi5zMiPIZ1Kk53rnxjCDFAgAs33Ew6vM/v9hY/95K82e1y8wND5qV5Ko/X9YXn912VsLVYNHDsjInolwA7wK4VQhhOWhaCPGsEKJUCFFaWKhfvyTZueK0DpbXlUvfOlFiVio5BpkYlJ+djgn9oxM3ZJEn9G+HTQ+fj04JGB7mFbJrJTWF6pVfm7wsNMlI9cztEhROxv3Z0kS5lRDOrPRUdLfgIk0ULI0CIkpHRJG/KoR4T1q8m4iKpO+LAFgvudYIsVMuVPbBO1Pm8f/WD8z+NqWbyK2a1YmMfKMTQtTPgchHyOsmHHr5Dve8v8KV7aulj/fP6ViQjSevikT9NGvS+KKurUSzEIDnAawWQjym+OpDABOl9xMBTHNfvOTBzgC9fUx3AM4mZtTlc8NWCU8Ic4V+ycD2SEshTOgfTCPvMCErbOUhky30oG7caveKFlaGnTynIxNvev/tY7rXz7Mkix/cDlYs82EArgEwkoiWSP/PB/AIgDFEtB7AGOkzo0PzbOtp2HLkQrzK/HhVLS566uuoZVqbctLhxSkCwvTv69wyBxsePj+q0FSjRTEpLh+2dEmLW20q/OJ1p8VVI98rQ6CuTuAfX2zA56t3u7I95RNcmBot+4Xps4gQ4ivoR6slT560x6SmEDq3zDGM6/74luHYvPdo/UCM9xq6693YmGCtTa3f4zwELF5C9qAQemRLs2lWGs7uXoh+7fMw6exI4lRORlpUJ3k9Soubo+ye0bjwqa9s5TBYjaoSQmDM36xV8gSA2Wv24E+fuDfJHm+SUrKQXDMnIeeWUcZNKk5t2wzj+xYpolniY3kIKx6qEYhV6I0sD8gWQ7oU4Pfn98TDF/dB85wMTPvNmejZJhJJdUyRUDRKJ0IIiBzz5jkZtpPRnphlrVbLK99ujWrrZ4ZWlJUTGrt9wMrcR6y6Whomu+Lbj7q6Xa+2zULpM0+WZA0/ICJMOqurZhjdb89rqCnSQqeqYtu8LDQxaeahx6crrblB5q7bG7PMzzNsNJ6y0pNf1SX/XxgirKagN0QnuHMpHK+utbylkzX+lL0VIjZGJRm6vQTBNUOL8eilfQDoGwBfTxmJ9NT4LnerhoCW79uo/rrrT2IGYipDEAtyvC8jHASszH3E6pyMvJpbNSN2HDiOtRZL8B485l3zAyUCQNs8dwqSMYpa9XrfO9CcTobhxBcW4KbXFvtSG98ohFVZ1fHKwdZzPhIJVuY+YjUeuKG4vzvavCAnAxc8+ZWlda99YYEr+zRDCIEzFa27GHfQsoStlgvWxeEwnL5sJ17V6BurV+rWymUytEuLmGVGl0uLJLXGlbAy9xGrxpEbtVmU7LRRHzqeJhrxoPWn8QRo/HRtlQsA9aWClThNKnJjGMrtDYUQeO27bdiw5zD+8NEq7f1Z2KHWn2T0M68Tq8JA40uTChCrA0pez8t+j0HDc5/uMqhTc8y76xy0b94E//NBdGbmoRPhGUeLtx3QbXUoE+/EuNHvlLXyk3XssTL3Eau2QfLbEMCto0tiljWGv9tL9FqdhSEbUrZjjleZhyNaeSLVtMwNfhfVect88wkJu1l8RE4GMptNbwyPhAM6NtpeJr5R2DRSSri61lk8txtzN/IErZWhbWV/WqtYldNKb91EhJW5j9QHHJoNuuTX5YwPPHP1QABAq2bWm2Broc5b8BorbpZ4XDHf3j0KXVrm4JohneIRK/Swm8VHSKNYkkynFtmK9XwSKGQ0tlZwXjOoUwGeumoARvVsHbQo9VizzM3X0TKuzTJK2+RlYfadI8w3nqCwZe4jraTH3tM7F9Qv+/K35+Cvl/fD3N82dKZPZjfLeKdhcowtLujbNq7+s3ax6uKwkhhWY8ENEraM5jDAlrmPdCjIxuw7zoZAQ4p0xxbZ6NgieuIqeVU58OQVA/D4T/prfpfMf7ef9OuQj7Ua7QC9xE3dalSMTkZL33cpbNzVNVmZ+0yXwlxsk3yQHQq0MyCT2TJPSSGksNr2lBcmltZ3q9LitOLmWLjlgKv7tOrDdmtoa+1vZIjcSUHAyjwAzAppJbEuZ3ygRW4mWuRm6n7fqqmzCVEtai0q8yue/daV/SVpQIoj2GceIKzMo2msf7fveHCcfXdhs888BlbmASB3e9GLd7UySWSnQXRQjOhRiKuHdAxaDEaFF248v2O3uftULKzMAyBNSi3Wm7W3Ul3xon5tzVcKmJd+NhgPXtwHX08ZaWl9Dk30B63xddM5XR1t08xn7nZT7n1Hqyyv+9y1pa7uO6ywMg+A7IzIVMWAjvma31tRauobweDiAp01g6ddfhP8bmxPvPiz02K+Y/3tP1qH3Km1vu+IdeXqBhf2tW7MjD61cUyM8gRoAORmpmHG5OFRiUJKrFxWduycqT8fjIk+lbbV41cjtC2/FCLLk2eMO2gZC06fikb85QtHv7cLGwGxsDIPiFOKmul+Z2Wg1tnwUfZtl2d5Xb/ha9J/1Md8WLcWCXcekjl8N17YzRJC9KykSwe2r3+vZ9Wr+fmwzmiek4F5d52Dywa1N/+Bz6RYbb/EuIfqkLfLb+K5cjxR7W7z5hTWXDHwIUkQlt13Lu4Zf0r95y6FuVHf600wyVlxHQqykdckthlw0KSzMvcddbSUEPG5LUpa5ZqvJPHYzHX2d2CAlYivM7rGdiNKZkzdLET0AoALAOwRQvSWlhUAeBNAMYAtAH4shHA3pYyJollWfIpYWcPCK73Zr0M+lm6vjOu3aakpAPxpIs1EUCtugfjGRpCeDiv7fulng31rUB4GrFjmLwEYq1o2BcAsIUQJgFnSZyZA9FzoOZkN92uvXRotczPqY+itksaWue+oj3jEMrd/HoL0W3eUGnHINdu1yEhLQdM4jaBExFSZCyG+BLBftXgCgKnS+6kALnZXLMYtlNdbqscXnxANlSGHdLEWKpmWysrcb2QlLDdJERBxKeZ427vFg7oh84COzTFj8nC0b65d36gxEq/PvLUQYicASK+t9FYkoklEVEZEZRUVFXHujjEjX/KH//T06IxL5fXWzGWfec82TaP3BaC11AjhznN7WNpGGs9k+Y58yOuzKAVQlGe/XosflrmcQZyb1fCEObJnRN2cUtSs3nhgfJgAFUI8K4QoFUKUFhbGdg5n7HHnud1xr2IiVCYrIxUbHz4fk0dF99ZUul+uHOwstX7SWV2iPss3isy0yDAac0pDcoZVl046W+a+U98kRTqBAsCE/m3x1FUD4tqOV+Rnp+Omc7oBAG4Y3jD2lPNA//jpIAwvaYmPbj7TU1kSgXiV+W4iKgIA6XWPeyIxRvxmZAl+MbxLzPK6OoHUFIpRospHYafRLFPG9oz6LEfQZGek4u0bh+LeC06xnbRt18fOOEd2t8k3eiEEiAgX2MiqtEtJq1zbDSWEAIrymmDtg2Px09M7Ilea/1FuJTWF8PL1p6N3iHMp/CJeZf4hgInS+4kAprkjDhMvsnWs9ou72ZFFfaNoKZVZFQI4rbggMtkk7c+qiuZ6LP4jn0Z5ZPjh+a6urcMzczeZrvfIj/rUv5cNkcy0VBARnpSeHLj8rTamypyIXgcwH0APIionousBPAJgDBGtBzBG+swEyH0X9QIQ68c8fEK/SYFTrj+zMwBtZWBVSXP7L/+Rb8rysa88Vh3XdlJNtMcTV/TH01dFmkoXNs3E/E37TLc5oX+7+vfFLaIrI6ZLzn7ZcGGiMY0zF0JcqfPVKJdlYXS4YXhnzeVpKVRfcCs/OzLbT6pxrgxNBIA+7fKwfMdB2zJoXUDyttvlN0yesWoOP/LTm/xkpVfwzYxrhnTC795drvt9UV4TDO5cgKnzCyzFsU8eVRI1h/Lny/tGfX9G1xb4zTndcN2w4rjkTXa4NkvI+dc1g3Berzaa36WnpqCmLjopQu1muVyVwv/stYMw9I+zbcsxeXRJzLLTigtw6+gSXHV67MQqu1nCy5hTW+O5rzbj9jHd8asRXTGwY/P675pmpVl+mvtxaQccOFaNR2as0fxetvwzUlNwvLoWGSbaRiCSRPbRzWeiU4vsmBjxlBTCnedZi5JqjPDzSsgxUnVaMdpKN8vZ3QulDMsG2jTTDkGTH4f1GKXRXzE1hXDr6O5RbcjYaxJ+Tu/SAlseGY/e7fJwWnFB1CT0w5f0MfhlNESEIV30U+Zl33Z6KmHvkZPmG5QGT+92eY0q2cctWJmHHCPLNV3DaalcXav7CxFh8qgSDC9pGbW8QJWUoc7M7CHFlP+4tD26FOZg08Pna8okR7hYNbjZZx4u1OetdTPjOG4rGbwHj1dj675j+HKdcZ4JjwRnsDJPYLQuJKWVVVOnXanutjHd8c+rB0UtU29K/b3Mny7rh9l3jDCNI7dSCAkADsQ5+cb4g9m91ihxSL6xqw2FePfFGMPKPOQsK6/U/U5LmSsvrnvHn2rpt00z0+q7HwHAAxN6YUyc3VnsXpByOvY7Nw6Na3+Mu8RUVDRZ3yhPQB4Ln6+2lobidmu5xgZPgIacqlr9OtBa1rFykVEihVLpjzm1Nfq0z8Ojl/bB2N5FjpKL5AvYqptFjpLxuyEwo01MRUWT02JFmVuFLXNnsGUecox8kloXktXoEOVq902IxKj/5LSOvtc879U2csNJ59jhUBA7pIw1rJEyt1uIi3W5M9gyDzmpBoWonBQ6Uv423cViV3YvyNtGd0f31k0xoEO+azIwTohtXKHFqVLbQ6NKnHbHAlvmzmBzKOQYXSzyV1qFt8ywchvoF4eClaNTrN5n8rLTcdXpHTnePCRkZ6RGfdbTr8UtI/XEUw0KpdnN1GSfuTNYmYcco0dVORZ3RI/YCsQtc40jCJS6U0+P3nO+/ZtE/TYTrkUwAwDDS1pGxZrrjb9BnSL16vWMjRY5GTi9s7Wa9jJVNe72CW1ssDIPOUbK/JmrB+KusT3QtTC6hsV/fj4Y/zUpCWrFEmZjufFBRFEZvVoRUfPuOgc/l1Lq9Tx0VwzuUD/GrCYisQHgDFbmIeXaoZ0AGEd5FOU1wa9HdItRzGd1L0RRnvMOLE4uLb4RJAeXqcpBDC9piQ4F2fVjTq+5iNIGyclM1VyHcRdW5iGljxRWKBdD8hI9xRuPH5snsZIbdQcpPTeLchhY7SblZxu6ZISjWULKpQPbIys9Fef3KfJ8X3rXUDzWtd10fiacTB5VglOkiBWZoV1axEyKK+sDXTm4A15fsB1A9Jiy2oCEcw2cwZZ5SElJIVzYr22gnXjk8MVebZuZrBkL+z8Tm9vGdMfY3tHVOrWiTeTol/F9inDb6O6a61qp3wIAtWyZO4ItcwZZ6do+zQ5Sqv2E/tbbifH1mLxoGc5EhKX/dy6yM1Jx4FhV/fIoy9xin9frzih2KGHjhpU5o0uL3EysfXAsMsxayiiQr2F2syQfB45WaS6Xs4aVT2PKaphmSWn92ufh6Z8ORPvm2S5I2XhhZc4YkpkWXyQC6/Lko2mWsbpQelPs+MyvHNyRFbkLsM+8EdOtVS5uHtnN1W1yffLk4/Gf9AcAFLfMMVxPGf2kHAVmY4LnPd2BLfNGzOe3n+3ZttnNkjxcPKAdWuRmoLSTcUanshSAMswwU2dORqZve/3qnox1WJkzrsJGVnIyvKTQdB3lRLrSGDcKORzft8iwVDNjHXazMO5Sf92yad6YOVHd0Gj8aJV+g+hHfmS95yhjDCtzxhPYzdK4UTZO6dc+X3c9btzsHo6UORGNJaK1RLSBiKa4JRSTuLCbhQGin8sKcjJw/ZmdA5OlsRC3MieiVABPAxgH4FQAVxKRftNJplFQX888YDmYYFHXWeG6K97jxDIfDGCDEGKTEKIKwBsAJrgjFpPocLOJxsnK+8/Deb1a41ZFaj/AmcF+4ESZtwOwXfG5XFoWBRFNIqIyIiqrqKhwsDsmEbiof2QIFGQbN8dgkpOczDT865pStG6WFbXcSYtDxhpOlLnW2Ym5/wohnhVClAohSgsLzcObmMTm1lElWHn/ecjL5oktpoGrh3TUaBbNuIkTZV4OoIPic3sAPzgTh0l0UlIIOZmcvsBE06Uw17T7FeMMJ8p8IYASIupMRBkArgDwoTtiMQyTbHBZZG+J24QSQtQQ0W8AfAogFcALQoiVrknGMExSIdc4b5mbiWuHdsJZ3dnt6iaOnoeFEB8D+NglWRiGSWLSpVLK+dnpuGVUScDSJB+cAcowjC/IES0cc+4NrMwZhvEFOZqFdbk3sDJnGMYX0qSOQ5lprHa8gGPIGIbxhQ4FTXD7mO64ZEBMbiHjAqzMGYbxBSLiiU8P4ecdhmGYJICVOcMwTBLAypxhGCYJYGXOMAyTBLAyZxiGSQJYmTMMwyQBrMwZhmGSAFbmDMMwSQAJHwslEFEFgK1x/rwlgL0uiuM1LK93JJKsAMvrJYkkKxC/vJ2EEIY1g31V5k4gojIhRGnQcliF5fWORJIVYHm9JJFkBbyVl90sDMMwSQArc4ZhmCQgkZT5s0ELYBOW1zsSSVaA5fWSRJIV8FDehPGZMwzDMPokkmXOMAzD6MDKnGEYJglICGVORGOJaC0RbSCiKT7u9wUi2kNEKxTLCohoJhGtl16bK767W5JxLRGdp1g+iIiWS9/9nSjS2ZaIMonoTWn5d0RU7EDWDkQ0h4hWE9FKIpoccnmziGgBES2V5L0/zPJK20slou+J6KMEkHWLtJ8lRFSWAPLmE9E7RLRGGsNDwyovEfWQjqv8/xAR3Rq4vEKIUP8HkApgI4AuADIALAVwqk/7PgvAQAArFMv+BGCK9H4KgEel96dKsmUC6CzJnCp9twDAUAAEYAaAcdLyXwN4Rnp/BYA3HchaBGCg9L4pgHWSTGGVlwDkSu/TAXwHYEhY5ZW2cTuA1wB8FOaxIG1jC4CWqmVhlncqgF9I7zMA5IdZXoXcqQB2AegUtLyeK0QXDtZQAJ8qPt8N4G4f91+MaGW+FkCR9L4IwFotuQB8KsleBGCNYvmVAP6lXEd6n4ZIZhi5JPc0AGMSQV4A2QAWAzg9rPICaA9gFoCRaFDmoZRV2sYWxCrzUMoLoBmAzerfh1VelYznAvg6DPImgpulHYDtis/l0rKgaC2E2AkA0msrabmenO2k9+rlUb8RQtQAOAighVMBpUeyAYhYu6GVV3JbLAGwB8BMIUSY5X0cwF0A6hTLwiorAAgAnxHRIiKaFHJ5uwCoAPCi5MZ6johyQiyvkisAvC69D1TeRFDmpLEsjPGUenIaye/630ZEuQDeBXCrEOKQ0ao6+/ZNXiFErRCiPyJW72Ai6m2wemDyEtEFAPYIIRZZ/YnOfv0cC8OEEAMBjANwExGdZbBu0PKmIeLO/KcQYgCAo4i4KfQIWt7IBokyAFwE4G2zVXX27aq8iaDMywF0UHxuD+CHgGQBgN1EVAQA0useabmenOXSe/XyqN8QURqAPAD74xWMiNIRUeSvCiHeC7u8MkKISgBfABgbUnmHAbiIiLYAeAPASCJ6JaSyAgCEED9Ir3sAvA9gcIjlLQdQLj2ZAcA7iCj3sMorMw7AYiHEbulzoPImgjJfCKCEiDpLd8IrAHwYoDwfApgovZ+IiG9aXn6FNAvdGUAJgAXS49ZhIhoizVRfq/qNvK3LAMwWkpPMLtK2nwewWgjxWALIW0hE+dL7JgBGA1gTRnmFEHcLIdoLIYoRGX+zhRBXh1FWACCiHCJqKr9HxK+7IqzyCiF2AdhORD2kRaMArAqrvAquRIOLRb0P/+V1OgHgx38A5yMSnbERwD0+7vd1ADsBVCNyp7weEb/VLADrpdcCxfr3SDKuhTQrLS0vReRi2gjgKTRk3mYh8oi2AZFZ7S4OZD0TkcewZQCWSP/PD7G8fQF8L8m7AsD/SstDKa9iXyPQMAEaSlkR8UEvlf6vlK+ZsMorba8/gDJpPHwAoHnI5c0GsA9AnmJZoPJyOj/DMEwSkAhuFoZhGMYEVuYMwzBJACtzhmGYJICVOcMwTBLAypxhGCYJYGXOMAyTBLAyZxiGSQL+H5pCpiEI5helAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(elc.OT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "5a7cc53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "4b44549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.tseries import offsets\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "\n",
    "\n",
    "class TimeFeature:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "class SecondOfMinute(TimeFeature):\n",
    "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.second / 59.0 - 0.5\n",
    "\n",
    "\n",
    "class MinuteOfHour(TimeFeature):\n",
    "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.minute / 59.0 - 0.5\n",
    "\n",
    "\n",
    "class HourOfDay(TimeFeature):\n",
    "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.hour / 23.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfWeek(TimeFeature):\n",
    "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.dayofweek / 6.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfMonth(TimeFeature):\n",
    "    \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.day - 1) / 30.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfYear(TimeFeature):\n",
    "    \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.dayofyear - 1) / 365.0 - 0.5\n",
    "\n",
    "\n",
    "class MonthOfYear(TimeFeature):\n",
    "    \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.month - 1) / 11.0 - 0.5\n",
    "\n",
    "\n",
    "class WeekOfYear(TimeFeature):\n",
    "    \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.isocalendar().week - 1) / 52.0 - 0.5\n",
    "\n",
    "\n",
    "def time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n",
    "    \"\"\"\n",
    "    Returns a list of time features that will be appropriate for the given frequency string.\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_str\n",
    "        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n",
    "    \"\"\"\n",
    "\n",
    "    features_by_offsets = {\n",
    "        offsets.YearEnd: [],\n",
    "        offsets.QuarterEnd: [MonthOfYear],\n",
    "        offsets.MonthEnd: [MonthOfYear],\n",
    "        offsets.Week: [DayOfMonth, WeekOfYear],\n",
    "        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Minute: [\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "        offsets.Second: [\n",
    "            SecondOfMinute,\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    offset = to_offset(freq_str)\n",
    "\n",
    "    for offset_type, feature_classes in features_by_offsets.items():\n",
    "        if isinstance(offset, offset_type):\n",
    "            return [cls() for cls in feature_classes]\n",
    "\n",
    "    supported_freq_msg = f\"\"\"\n",
    "    Unsupported frequency {freq_str}\n",
    "    The following frequencies are supported:\n",
    "        Y   - yearly\n",
    "            alias: A\n",
    "        M   - monthly\n",
    "        W   - weekly\n",
    "        D   - daily\n",
    "        B   - business days\n",
    "        H   - hourly\n",
    "        T   - minutely\n",
    "            alias: min\n",
    "        S   - secondly\n",
    "    \"\"\"\n",
    "    raise RuntimeError(supported_freq_msg)\n",
    "\n",
    "\n",
    "def time_features(dates, freq='h'):\n",
    "    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "7f098ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "3ad3fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Dataset_Custom(Dataset):\n",
    "    def __init__(self, flag='train', size=[192, 48, 96],\n",
    "                 features='S',\n",
    "                 target='OT', scale=True, timeenc=0, freq='h'):\n",
    "        # size [seq_len, label_len, pred_len]\n",
    "        # info\n",
    "        if size == None:\n",
    "            self.seq_len = 24 * 4 * 4\n",
    "            self.label_len = 24 * 4\n",
    "            self.pred_len = 24 * 4\n",
    "        else:\n",
    "            self.seq_len = size[0]\n",
    "            self.label_len = size[1]\n",
    "            self.pred_len = size[2]\n",
    "        # init\n",
    "        assert flag in ['train', 'test', 'val']\n",
    "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "        self.set_type = type_map[flag]\n",
    "\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.scale = scale\n",
    "        self.timeenc = timeenc\n",
    "        self.freq = freq\n",
    "\n",
    "        self.__read_data__()\n",
    "\n",
    "    def __read_data__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        df_raw = pd.read_csv('datasets/ETTm2.csv')\n",
    "\n",
    "        '''\n",
    "        df_raw.columns: ['date', ...(other features), target feature]\n",
    "        '''\n",
    "        cols = list(df_raw.columns)\n",
    "        cols.remove(self.target)\n",
    "        cols.remove('date')\n",
    "        df_raw = df_raw[['date'] + cols + [self.target]]\n",
    "        # print(cols)\n",
    "        num_train = int(len(df_raw) * 0.7)\n",
    "        num_test = int(len(df_raw) * 0.2)\n",
    "        num_vali = len(df_raw) - num_train - num_test\n",
    "        border1s = [0, num_train - self.seq_len, len(df_raw) - num_test - self.seq_len]\n",
    "        border2s = [num_train, num_train + num_vali, len(df_raw)]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "\n",
    "        if self.features == 'M' or self.features == 'MS':\n",
    "            cols_data = df_raw.columns[1:]\n",
    "            df_data = df_raw[cols_data]\n",
    "        elif self.features == 'S':\n",
    "            df_data = df_raw[[self.target]]\n",
    "\n",
    "        if self.scale:\n",
    "            train_data = df_data[border1s[0]:border2s[0]]\n",
    "            self.scaler.fit(train_data.values)\n",
    "            data = self.scaler.transform(df_data.values)\n",
    "        else:\n",
    "            data = df_data.values\n",
    "\n",
    "        df_stamp = df_raw[['date']][border1:border2]\n",
    "        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
    "        if self.timeenc == 0:\n",
    "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
    "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
    "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
    "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
    "            data_stamp = df_stamp.drop(['date'], 1).values\n",
    "        elif self.timeenc == 1:\n",
    "            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
    "            data_stamp = data_stamp.transpose(1, 0)\n",
    "\n",
    "        self.data_x = data[border1:border2]\n",
    "        self.data_y = data[border1:border2]\n",
    "        self.data_stamp = data_stamp\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "\n",
    "        seq_x = self.data_x[s_begin:s_end]\n",
    "        seq_y = self.data_y[r_begin:r_end]\n",
    "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
    "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
    "\n",
    "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9684588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "542bed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm\n",
    "import math\n",
    "\n",
    "def compared_version(ver1, ver2):\n",
    "    \"\"\"\n",
    "    :param ver1\n",
    "    :param ver2\n",
    "    :return: ver1< = >ver2 False/True\n",
    "    \"\"\"\n",
    "    list1 = str(ver1).split(\".\")\n",
    "    list2 = str(ver2).split(\".\")\n",
    "    \n",
    "    for i in range(len(list1)) if len(list1) < len(list2) else range(len(list2)):\n",
    "        if int(list1[i]) == int(list2[i]):\n",
    "            pass\n",
    "        elif int(list1[i]) < int(list2[i]):\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    if len(list1) == len(list2):\n",
    "        return True\n",
    "    elif len(list1) < len(list2):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        padding = 1 if compared_version(torch.__version__, '1.5.0') else 2\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
    "                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FixedEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(FixedEmbedding, self).__init__()\n",
    "\n",
    "        w = torch.zeros(c_in, d_model).float()\n",
    "        w.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, c_in).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        w[:, 0::2] = torch.sin(position * div_term)\n",
    "        w[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.emb = nn.Embedding(c_in, d_model)\n",
    "        self.emb.weight = nn.Parameter(w, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x).detach()\n",
    "\n",
    "\n",
    "class TemporalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='timeF', freq='h'):\n",
    "        super(TemporalEmbedding, self).__init__()\n",
    "\n",
    "        minute_size = 4\n",
    "        hour_size = 24\n",
    "        weekday_size = 7\n",
    "        day_size = 32\n",
    "        month_size = 13\n",
    "\n",
    "        Embed = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n",
    "        if freq == 't':\n",
    "            self.minute_embed = Embed(minute_size, d_model)\n",
    "        self.hour_embed = Embed(hour_size, d_model)\n",
    "        self.weekday_embed = Embed(weekday_size, d_model)\n",
    "        self.day_embed = Embed(day_size, d_model)\n",
    "        self.month_embed = Embed(month_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.long()\n",
    "\n",
    "        minute_x = self.minute_embed(x[:, :, 4]) if hasattr(self, 'minute_embed') else 0.\n",
    "        hour_x = self.hour_embed(x[:, :, 3])\n",
    "        weekday_x = self.weekday_embed(x[:, :, 2])\n",
    "        day_x = self.day_embed(x[:, :, 1])\n",
    "        month_x = self.month_embed(x[:, :, 0])\n",
    "\n",
    "        return hour_x + weekday_x + day_x + month_x + minute_x\n",
    "\n",
    "\n",
    "class TimeFeatureEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='timeF', freq='h'):\n",
    "        super(TimeFeatureEmbedding, self).__init__()\n",
    "\n",
    "        freq_map = {'h': 4, 't': 5, 's': 6, 'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n",
    "        d_inp = freq_map[freq]\n",
    "        self.embed = nn.Linear(d_inp, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)\n",
    "\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, embed_type='timeF', freq='h', dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
    "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
    "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_mark):\n",
    "        x = self.value_embedding(x) + self.temporal_embedding(x_mark) + self.position_embedding(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class DataEmbedding_wo_pos(nn.Module):\n",
    "    def __init__(self, c_in, d_model, embed_type='timeF', freq='h', dropout=0.1):\n",
    "        super(DataEmbedding_wo_pos, self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
    "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
    "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_mark):\n",
    "        x = self.value_embedding(x) + self.temporal_embedding(x_mark)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "70e996d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class my_Layernorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Special designed layernorm for the seasonal part\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(my_Layernorm, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = self.layernorm(x)\n",
    "        bias = torch.mean(x_hat, dim=1).unsqueeze(1).repeat(1, x.shape[1], 1)\n",
    "        return x_hat - bias\n",
    "\n",
    "\n",
    "class moving_avg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer encoder layer with the progressive decomposition architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, attention, d_model, d_ff=None, moving_avg=25, dropout=0.1, activation=\"relu\"):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.attention = attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1, bias=False)\n",
    "        self.decomp1 = series_decomp(moving_avg)\n",
    "        self.decomp2 = series_decomp(moving_avg)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        new_x, attn = self.attention(\n",
    "            x, x, x,\n",
    "            attn_mask=attn_mask\n",
    "        )\n",
    "        x = x + self.dropout(new_x)\n",
    "        x, _ = self.decomp1(x)\n",
    "        y = x\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "        res, _ = self.decomp2(x + y)\n",
    "        return res, attn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attn_layers = nn.ModuleList(attn_layers)\n",
    "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
    "        self.norm = norm_layer\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        attns = []\n",
    "        if self.conv_layers is not None:\n",
    "            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                x = conv_layer(x)\n",
    "                attns.append(attn)\n",
    "            x, attn = self.attn_layers[-1](x)\n",
    "            attns.append(attn)\n",
    "        else:\n",
    "            for attn_layer in self.attn_layers:\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                attns.append(attn)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x, attns\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer decoder layer with the progressive decomposition architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, self_attention, cross_attention, d_model, c_out, d_ff=None,\n",
    "                 moving_avg=25, dropout=0.1, activation=\"relu\"):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.self_attention = self_attention\n",
    "        self.cross_attention = cross_attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1, bias=False)\n",
    "        self.decomp1 = series_decomp(moving_avg)\n",
    "        self.decomp2 = series_decomp(moving_avg)\n",
    "        self.decomp3 = series_decomp(moving_avg)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.projection = nn.Conv1d(in_channels=d_model, out_channels=c_out, kernel_size=3, stride=1, padding=1,\n",
    "                                    padding_mode='circular', bias=False)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None):\n",
    "        x = x + self.dropout(self.self_attention(\n",
    "            x, x, x,\n",
    "            attn_mask=x_mask\n",
    "        )[0])\n",
    "        x, trend1 = self.decomp1(x)\n",
    "        x = x + self.dropout(self.cross_attention(\n",
    "            x, cross, cross,\n",
    "            attn_mask=cross_mask\n",
    "        )[0])\n",
    "        x, trend2 = self.decomp2(x)\n",
    "        y = x\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "        x, trend3 = self.decomp3(x + y)\n",
    "\n",
    "        residual_trend = trend1 + trend2 + trend3\n",
    "        residual_trend = self.projection(residual_trend.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x, residual_trend\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, layers, norm_layer=None, projection=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.norm = norm_layer\n",
    "        self.projection = projection\n",
    "\n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None, trend=None):\n",
    "        for layer in self.layers:\n",
    "            x, residual_trend = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)\n",
    "            trend = trend + residual_trend\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        if self.projection is not None:\n",
    "            x = self.projection(x)\n",
    "        return x, trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "da130378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from math import sqrt\n",
    "import os\n",
    "\n",
    "\n",
    "class AutoCorrelation(nn.Module):\n",
    "    \"\"\"\n",
    "    AutoCorrelation Mechanism with the following two phases:\n",
    "    (1) period-based dependencies discovery\n",
    "    (2) time delay aggregation\n",
    "    This block can replace the self-attention family mechanism seamlessly.\n",
    "    \"\"\"\n",
    "    def __init__(self, mask_flag=True, factor=1, scale=None, attention_dropout=0.1, output_attention=False):\n",
    "        super(AutoCorrelation, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def time_delay_agg_training(self, values, corr):\n",
    "        \"\"\"\n",
    "        SpeedUp version of Autocorrelation (a batch-normalization style design)\n",
    "        This is for the training phase.\n",
    "        \"\"\"\n",
    "        head = values.shape[1]\n",
    "        channel = values.shape[2]\n",
    "        length = values.shape[3]\n",
    "        # find top k\n",
    "        top_k = int(self.factor * math.log(length))\n",
    "        mean_value = torch.mean(torch.mean(corr, dim=1), dim=1)\n",
    "        index = torch.topk(torch.mean(mean_value, dim=0), top_k, dim=-1)[1]\n",
    "        weights = torch.stack([mean_value[:, index[i]] for i in range(top_k)], dim=-1)\n",
    "        # update corr\n",
    "        tmp_corr = torch.softmax(weights, dim=-1)\n",
    "        # aggregation\n",
    "        tmp_values = values\n",
    "        delays_agg = torch.zeros_like(values).float()\n",
    "        for i in range(top_k):\n",
    "            pattern = torch.roll(tmp_values, -int(index[i]), -1)\n",
    "            delays_agg = delays_agg + pattern * \\\n",
    "                         (tmp_corr[:, i].unsqueeze(1).unsqueeze(1).unsqueeze(1).repeat(1, head, channel, length))\n",
    "        return delays_agg\n",
    "\n",
    "    def time_delay_agg_inference(self, values, corr):\n",
    "        \"\"\"\n",
    "        SpeedUp version of Autocorrelation (a batch-normalization style design)\n",
    "        This is for the inference phase.\n",
    "        \"\"\"\n",
    "        batch = values.shape[0]\n",
    "        head = values.shape[1]\n",
    "        channel = values.shape[2]\n",
    "        length = values.shape[3]\n",
    "        # index init\n",
    "        init_index = torch.arange(length).unsqueeze(0).unsqueeze(0).unsqueeze(0)\\\n",
    "            .repeat(batch, head, channel, 1).to(values.device)\n",
    "        # find top k\n",
    "        top_k = int(self.factor * math.log(length))\n",
    "        mean_value = torch.mean(torch.mean(corr, dim=1), dim=1)\n",
    "        weights, delay = torch.topk(mean_value, top_k, dim=-1)\n",
    "        # update corr\n",
    "        tmp_corr = torch.softmax(weights, dim=-1)\n",
    "        # aggregation\n",
    "        tmp_values = values.repeat(1, 1, 1, 2)\n",
    "        delays_agg = torch.zeros_like(values).float()\n",
    "        for i in range(top_k):\n",
    "            tmp_delay = init_index + delay[:, i].unsqueeze(1).unsqueeze(1).unsqueeze(1).repeat(1, head, channel, length)\n",
    "            pattern = torch.gather(tmp_values, dim=-1, index=tmp_delay)\n",
    "            delays_agg = delays_agg + pattern * \\\n",
    "                         (tmp_corr[:, i].unsqueeze(1).unsqueeze(1).unsqueeze(1).repeat(1, head, channel, length))\n",
    "        return delays_agg\n",
    "\n",
    "    def time_delay_agg_full(self, values, corr):\n",
    "        \"\"\"\n",
    "        Standard version of Autocorrelation\n",
    "        \"\"\"\n",
    "        batch = values.shape[0]\n",
    "        head = values.shape[1]\n",
    "        channel = values.shape[2]\n",
    "        length = values.shape[3]\n",
    "        # index init\n",
    "        init_index = torch.arange(length).unsqueeze(0).unsqueeze(0).unsqueeze(0)\\\n",
    "            .repeat(batch, head, channel, 1).to(values.device)\n",
    "        # find top k\n",
    "        top_k = int(self.factor * math.log(length))\n",
    "        weights, delay = torch.topk(corr, top_k, dim=-1)\n",
    "        # update corr\n",
    "        tmp_corr = torch.softmax(weights, dim=-1)\n",
    "        # aggregation\n",
    "        tmp_values = values.repeat(1, 1, 1, 2)\n",
    "        delays_agg = torch.zeros_like(values).float()\n",
    "        for i in range(top_k):\n",
    "            tmp_delay = init_index + delay[..., i].unsqueeze(-1)\n",
    "            pattern = torch.gather(tmp_values, dim=-1, index=tmp_delay)\n",
    "            delays_agg = delays_agg + pattern * (tmp_corr[..., i].unsqueeze(-1))\n",
    "        return delays_agg\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, H, E = queries.shape\n",
    "        _, S, _, D = values.shape\n",
    "        if L > S:\n",
    "            zeros = torch.zeros_like(queries[:, :(L - S), :]).float()\n",
    "            values = torch.cat([values, zeros], dim=1)\n",
    "            keys = torch.cat([keys, zeros], dim=1)\n",
    "        else:\n",
    "            values = values[:, :L, :, :]\n",
    "            keys = keys[:, :L, :, :]\n",
    "\n",
    "        # period-based dependencies\n",
    "        q_fft = torch.fft.rfft(queries.permute(0, 2, 3, 1).contiguous(), dim=-1)\n",
    "        k_fft = torch.fft.rfft(keys.permute(0, 2, 3, 1).contiguous(), dim=-1)\n",
    "        res = q_fft * torch.conj(k_fft)\n",
    "        corr = torch.fft.irfft(res, dim=-1)\n",
    "\n",
    "        # time delay agg\n",
    "        if self.training:\n",
    "            V = self.time_delay_agg_training(values.permute(0, 2, 3, 1).contiguous(), corr).permute(0, 3, 1, 2)\n",
    "        else:\n",
    "            V = self.time_delay_agg_inference(values.permute(0, 2, 3, 1).contiguous(), corr).permute(0, 3, 1, 2)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return (V.contiguous(), corr.permute(0, 3, 1, 2))\n",
    "        else:\n",
    "            return (V.contiguous(), None)\n",
    "\n",
    "\n",
    "class AutoCorrelationLayer(nn.Module):\n",
    "    def __init__(self, correlation, d_model, n_heads, d_keys=None,\n",
    "                 d_values=None):\n",
    "        super(AutoCorrelationLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "        d_values = d_values or (d_model // n_heads)\n",
    "\n",
    "        self.inner_correlation = correlation\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "\n",
    "        out, attn = self.inner_correlation(\n",
    "            queries,\n",
    "            keys,\n",
    "            values,\n",
    "            attn_mask\n",
    "        )\n",
    "        out = out.view(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out), attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "8f516c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univaraite case\n",
    "#   --features S \\\n",
    "#   --seq_len 96 \\\n",
    "#   --label_len 96 \\\n",
    "#   --pred_len 192 \\\n",
    "#   --e_layers 2 \\\n",
    "#   --d_layers 1 \\\n",
    "#   --factor 3 \\\n",
    "#   --enc_in 1 \\\n",
    "#   --dec_in 1 \\\n",
    "#   --c_out 1 \\\n",
    "#   --des 'Exp' \\\n",
    "#   --itr 1 \\\n",
    "#   --freq 't' \\\n",
    "#   --train_epochs 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "b194b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer is the first method to achieve the series-wise connection,\n",
    "    with inherent O(LlogL) complexity\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.seq_len = 96\n",
    "        self.label_len = 96\n",
    "        self.pred_len = 192\n",
    "        self.output_attention = True\n",
    "\n",
    "        # Decomp\n",
    "        self.e_layers = 2\n",
    "        self.d_layers = 1\n",
    "        self.factor = 3\n",
    "        self.kernel_size = 25\n",
    "        self.moving_avg  = 25\n",
    "        self.activation = 'gelu'\n",
    "        self.decomp = series_decomp(self.kernel_size)\n",
    "        self.dropout = 0.05\n",
    "        self.enc_in = 1\n",
    "        self.dec_in = 1\n",
    "        self.freq = 't'\n",
    "        self.embed = 'timeF'\n",
    "        self.d_model = 512\n",
    "        self.n_heads = 8\n",
    "        self.c_out = 1\n",
    "        self.d_ff = 2048\n",
    "#         self.decomp = series_decomp(kernel_size)\n",
    "\n",
    "        # Embedding\n",
    "        # The series-wise connection inherently contains the sequential information.\n",
    "        # Thus, we can discard the position embedding of transformers.\n",
    "        self.enc_embedding = DataEmbedding_wo_pos(self.enc_in, self.d_model, self.embed, self.freq,\n",
    "                                                  self.dropout)\n",
    "        self.dec_embedding = DataEmbedding_wo_pos(self.dec_in, self.d_model, self.embed, self.freq,\n",
    "                                                  self.dropout)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AutoCorrelationLayer(\n",
    "                        AutoCorrelation(False, self.factor, attention_dropout=self.dropout,\n",
    "                                        output_attention=self.output_attention),\n",
    "                        self.d_model, self.n_heads),\n",
    "                    self.d_model,\n",
    "                    self.d_ff,\n",
    "                    moving_avg=self.moving_avg,\n",
    "                    dropout=self.dropout,\n",
    "                    activation=self.activation\n",
    "                ) for l in range(self.e_layers)\n",
    "            ],\n",
    "            norm_layer=my_Layernorm(self.d_model)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            [\n",
    "                DecoderLayer(\n",
    "                    AutoCorrelationLayer(\n",
    "                        AutoCorrelation(True, self.factor, attention_dropout=self.dropout,\n",
    "                                        output_attention=False),\n",
    "                        self.d_model, self.n_heads),\n",
    "                    AutoCorrelationLayer(\n",
    "                        AutoCorrelation(False, self.factor, attention_dropout=self.dropout,\n",
    "                                        output_attention=False),\n",
    "                        self.d_model, self.n_heads),\n",
    "                    self.d_model,\n",
    "                    self.c_out,\n",
    "                    self.d_ff,\n",
    "                    moving_avg=self.moving_avg,\n",
    "                    dropout=self.dropout,\n",
    "                    activation=self.activation,\n",
    "                )\n",
    "                for l in range(self.d_layers)\n",
    "            ],\n",
    "            norm_layer=my_Layernorm(self.d_model),\n",
    "            projection=nn.Linear(self.d_model, self.c_out, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec,\n",
    "                enc_self_mask=None, dec_self_mask=None, dec_enc_mask=None):\n",
    "        # decomp init\n",
    "        mean = torch.mean(x_enc, dim=1).unsqueeze(1).repeat(1, self.pred_len, 1)\n",
    "        zeros = torch.zeros([x_dec.shape[0], self.pred_len, x_dec.shape[2]], device=x_enc.device)\n",
    "        seasonal_init, trend_init = self.decomp(x_enc)\n",
    "        # decoder input\n",
    "        trend_init = torch.cat([trend_init[:, -self.label_len:, :], mean], dim=1)\n",
    "        seasonal_init = torch.cat([seasonal_init[:, -self.label_len:, :], zeros], dim=1)\n",
    "        # enc\n",
    "        enc_out = self.enc_embedding(x_enc.float(), x_mark_enc.float())\n",
    "        enc_out, attns = self.encoder(enc_out, attn_mask=enc_self_mask)\n",
    "        # dec\n",
    "        dec_out = self.dec_embedding(seasonal_init.float(), x_mark_dec.float())\n",
    "        seasonal_part, trend_part = self.decoder(dec_out, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask,\n",
    "                                                 trend=trend_init)\n",
    "        # final\n",
    "        dec_out = trend_part + seasonal_part\n",
    "\n",
    "        if self.output_attention:\n",
    "            return dec_out[:, -self.pred_len:, :], attns\n",
    "        else:\n",
    "            return dec_out[:, -self.pred_len:, :]  # [B, L, D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "aa22cebc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ED = elc\n",
    "\n",
    "    \n",
    "ED['d'] = pd.to_datetime(ED['date'])\n",
    "ED['Year'] = ED['date'].apply(lambda x: int(x[:10][:4]))\n",
    "ED['month'] = ED['date'].apply(lambda x: int(x[:10][5:7]))\n",
    "ED['hr'] = ED['date'].apply(lambda x: int(x[11:][:2]))\n",
    "ED['min'] = ED['date'].apply(lambda x: int(x[11:][3:5]))\n",
    "ED['day'] = ED.d.apply(lambda row: row.day, 1)\n",
    "ED['weekday'] = ED.d.apply(lambda row: row.weekday(), 1)\n",
    "\n",
    "ED['t'] = ED['Year'] + ((ED['month'] - 1)/ 12) + ((ED['day'] - 1)/ (12 *31) + ED['hr']/(24*31*12) + ED['min']/(60*24*31*12))\n",
    "t = np.array(ED['t'])\n",
    "ED_13 = (ED.loc[ED.t >= 2013, :])\n",
    "\n",
    "client_w_time = (ED.loc[ED.t >= 2013, ['Year', 'month', 'day', 'weekday', 'hr','min', 'OT']])\n",
    "time_feat = ED.loc[ED.t >= 2013, ['Year', 'month', 'day', 'weekday', 'hr','min']]\n",
    "\n",
    "\n",
    "t_13 = ED_13.index\n",
    "train_last_ix = int(len(t_13) *0.8)\n",
    "# target = ED_13.loc[:, ['HUFL','HULL','LUFL','LULL','MUFL', 'MULL', 'OT']]\n",
    "target = ED_13.loc[:, ['OT']]\n",
    "arr_kwh = target \n",
    "μ_arr =  np.mean(arr_kwh[:train_last_ix], axis=0)\n",
    "σ_arr =  np.std(arr_kwh[:train_last_ix], axis=0)\n",
    "arr_kwh_a = np.array((arr_kwh - μ_arr)/σ_arr)\n",
    "# time_features = client_w_time.iloc[:, 1:6]\n",
    "time_features = client_w_time.iloc[:, 1:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "da1aef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_target = arr_kwh_a[:int(0.7*arr_kwh_a.shape[0])]\n",
    "val_data_target = arr_kwh_a[int(0.7*arr_kwh_a.shape[0]):int(0.8*arr_kwh_a.shape[0])]\n",
    "test_data_target = arr_kwh_a[int(0.8*arr_kwh_a.shape[0]):]\n",
    "\n",
    "\n",
    "training_data_time = time_features.iloc[:int(0.7*arr_kwh_a.shape[0]), :]\n",
    "val_data_time = time_features.iloc[int(0.7*arr_kwh_a.shape[0]):int(0.8*arr_kwh_a.shape[0]), :]\n",
    "test_data_time = time_features.iloc[int(0.8*arr_kwh_a.shape[0]):, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "b3e9f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batcher(time_features, target, batch_s = 32, training=True, tar_dim = 1):\n",
    "    enc_l = 96\n",
    "    dec_l = 192\n",
    "    win_l = enc_l + dec_l\n",
    "    idx = np.random.choice(np.arange(win_l, len(target) - win_l), batch_s)\n",
    "    tar = np.array([target[i:i+win_l] for i in idx])\n",
    "    t = np.array([np.array(time_features)[i:i+win_l, :] for i in idx])    \n",
    "    x_enc = tar[:, :enc_l].reshape(batch_s, enc_l, tar_dim); \n",
    "    x_dec = tar[:, :win_l].reshape(batch_s, win_l, tar_dim)\n",
    "    \n",
    "    y_tar = x_dec[:, -192:, :].copy()\n",
    "    x_dec[:, -192:, :] = 0\n",
    "    \n",
    "    x_mark_enc = t[:, :enc_l, :]; \n",
    "    x_mark_dec = t[:, :win_l, :]\n",
    "    \n",
    "    return x_enc, x_dec, x_mark_enc, x_mark_dec, y_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "c88a35a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "26431873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "88a2d60a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [327]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m model_optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# decoder input\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x_mark\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_tar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_y_mark\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m192\u001b[39m:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     22\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y_tar[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m192\u001b[39m:, \u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [305]\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec, enc_self_mask, dec_self_mask, dec_enc_mask)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# dec\u001b[39;00m\n\u001b[1;32m     95\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec_embedding(seasonal_init\u001b[38;5;241m.\u001b[39mfloat(), x_mark_dec\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m---> 96\u001b[0m seasonal_part, trend_part \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdec_self_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcross_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdec_enc_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mtrend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrend_init\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# final\u001b[39;00m\n\u001b[1;32m     99\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m trend_part \u001b[38;5;241m+\u001b[39m seasonal_part\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [300]\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x, cross, x_mask, cross_mask, trend)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, cross, x_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cross_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, trend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 165\u001b[0m         x, residual_trend \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcross\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcross_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m         trend \u001b[38;5;241m=\u001b[39m trend \u001b[38;5;241m+\u001b[39m residual_trend\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [300]\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, x, cross, x_mask, cross_mask)\u001b[0m\n\u001b[1;32m    142\u001b[0m x, trend2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecomp2(x)\n\u001b[1;32m    143\u001b[0m y \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m--> 144\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    145\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(y)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    146\u001b[0m x, trend3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecomp3(x \u001b[38;5;241m+\u001b[39m y)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/torch/nn/modules/conv.py:313\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/torch/nn/modules/conv.py:309\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    307\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    308\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_models = []\n",
    "for j in range(5):\n",
    "    mini = 5000\n",
    "    model = Model()\n",
    "    model_optim = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    criterion = nn.MSELoss()\n",
    "    iter_count = 0\n",
    "    epoch_time = time.time()\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for i in range(40000):\n",
    "        print(i)\n",
    "        \n",
    "        model.train()\n",
    "        batch_x, batch_y, batch_x_mark, batch_y_mark, y_tar = batcher(np.array(training_data_time), training_data_target) \n",
    "        iter_count += 1\n",
    "        model_optim.zero_grad()\n",
    "        # decoder input\n",
    "        outputs = model(torch.tensor(batch_x), torch.tensor(batch_x_mark), torch.tensor(y_tar), torch.tensor(batch_y_mark))\n",
    "\n",
    "        outputs = outputs[0][:, -192:, 0]\n",
    "        batch_y = torch.tensor(y_tar[:, -192:, 0])\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        model_optim.step()\n",
    "        if (i + 1) % 500 == 0:\n",
    "            # evaluate model:\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_x, batch_y, batch_x_mark, batch_y_mark, y_tar = batcher(np.array(val_data_time), val_data_target, batch_s=100, training=False) \n",
    "                out_data = model(torch.tensor(batch_x), torch.tensor(batch_x_mark), torch.tensor(y_tar), torch.tensor(batch_y_mark))\n",
    "                out_data = out_data[0][:, -192:, 0]\n",
    "                batch_y = torch.tensor(y_tar[:, -192:, 0])\n",
    "                loss_v = criterion(out_data, batch_y)\n",
    "                val_loss.append(loss_v.item())\n",
    "                IPython.display.clear_output(wait=True)\n",
    "                plt.plot(train_loss, label = 'train mse')\n",
    "                plt.plot(val_loss, label= 'validation mse')\n",
    "                plt.show()\n",
    "                if loss_v < mini:\n",
    "                    mini = loss_v\n",
    "                    torch.save(model.state_dict(), 'weights/forecasting/ETT/autoformer/96_context_192_pred/192_out_best-model-parameters_{}.pt'.format(j)) # official recommended\n",
    "\n",
    "            print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, i + 1, loss.item()))\n",
    "            print(\"\\titers: {0}, epoch: {1} | loss val: {2:.7f}\".format(i + 1, i + 1, loss_v.item()))\n",
    "            print(\"iter: {} cost time: {}\".format(iter_count + 1, time.time() - epoch_time))\n",
    "            epoch_time = time.time()\n",
    "    \n",
    "    best_models.append(np.min(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "99faa355",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_size=0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "1c8b3f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42024964"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2042dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = np.load('merged_192_out_96_input.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c0d7ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = np.load('x_all_test_set_electricity_192_out.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a055f0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "96+192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e02ef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_delete = np.array([33, 46, 54, 71, 82, 105, 166, 188, 200, 201, 218, 229, 269, 289, 302, 330, 429, 450, 491, 492, 495, 498, 519, 521, 549, 561, 595, 602, 613, 619, 622, 663, 678, 708, 725, 745, 817, 866, 868, 870, 890, 911, 916, 925, 926, 945, 974,\n",
    "891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 912, 913, 914, 915, 917, 918, 919, 920, 921, 922, 923, 924, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "faf5d77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = np.delete(merged, rows_to_delete, axis=0)\n",
    "x_all = np.delete(x_all, rows_to_delete, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0c6f44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_l = 96\n",
    "win_l = 288\n",
    "batch_s = 850"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e68784e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    x_enc = merged[:, :enc_l, :].reshape(batch_s, enc_l, 1); \n",
    "\n",
    "    x_dec = merged[:, :win_l, :].reshape(batch_s, win_l, 1)\n",
    "    y_tar = merged[:, -192:, :].copy()\n",
    "    x_dec[:, -192:, :] = 0\n",
    "    \n",
    "    x_mark_enc = x_all[:, :enc_l, :]; \n",
    "    x_mark_dec = x_all[:, :win_l, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1293fb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "import math as ma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9103653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_lik(y, ŷ, log_σ, ϵ=0.001):\n",
    "    pi = torch.tensor(ma.pi)\n",
    "    mse_per_point = torch.square(torch.subtract(y, ŷ))\n",
    "    lik_per_point = (-1 / 2) * (torch.divide(mse_per_point, torch.square(torch.exp(log_σ)) + ϵ)) - 1*torch.log(torch.exp(log_σ) + ϵ)- (1/2)*torch.log(2*pi)\n",
    "    return lik_per_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07266fc7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m like_test_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m() \u001b[38;5;66;03m# we do not specify pretrained=True, i.e. do not load default weights\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m192_out_best-model-parameters_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i)))\n\u001b[1;32m      6\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Model' is not defined"
     ]
    }
   ],
   "source": [
    "mse_test_loss = []\n",
    "like_test_loss = []\n",
    "for i in range(5):\n",
    "    model = Model() # we do not specify pretrained=True, i.e. do not load default weights\n",
    "    model.load_state_dict(torch.load('192_out_best-model-parameters_{}.pt'.format(i)))\n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "    test_batch_s = 100 #need to specify this as it gets changed in the loop below\n",
    "    idx_list = list(range(test_data_time.shape[0] - (192)))\n",
    "    num_batches = len(idx_list)//test_batch_s\n",
    "\n",
    "    for _ in range(num_batches): #### specify correct number of batches for the batcher #####\n",
    "        if(_ == (num_batches-1)): test_batch_s = len(idx_list)        \n",
    "        batch_x, batch_y, batch_x_mark, batch_y_mark, y_tar = batcher_test(np.array(test_data_time), test_data_target, batch_s=100, training=False) \n",
    "        outputs_te = model(torch.tensor(x_enc), torch.tensor(x_mark_enc), torch.tensor(y_tar), torch.tensor(x_mark_dec))\n",
    "        batch_y = torch.tensor(y_tar)\n",
    "        loss = criterion(outputs_te[0], batch_y)\n",
    "        \n",
    "        _, sum_mse, sum_nll, _, _ = losses.nll(y_te[:, n_C:n_C+n_T], μ, log_σ)\n",
    "        sum_nll_tot += sum_nll / n_T\n",
    "        sum_mse_tot += sum_mse / n_T\n",
    "\n",
    "    nllx =  sum_nll_tot / (test_batch_s * x_test.shape[0]//test_batch_s)\n",
    "    msex =  sum_mse_tot / (test_batch_s * x_test.shape[0]//test_batch_s)\n",
    "\n",
    "\n",
    "    nll_list.append(nllx.numpy())\n",
    "    mse_list.append(msex.numpy())\n",
    "                \n",
    "            \n",
    "    np.save(save_dir + '/nll_list.npy', nll_list)    \n",
    "    np.save(save_dir + '/mse_list.npy', mse_list)  \n",
    "\n",
    "    mse_test_loss.append(loss.detach().numpy())\n",
    "    log_σ = np.log(np.sqrt(mse_test_loss[-1]))\n",
    "    lik_pp = seq_lik(batch_y, outputs_te[0], torch.tensor(log_σ))\n",
    "    like_test_loss.append(np.mean(lik_pp.detach().numpy()))\n",
    "print(np.mean(mse_test_loss))\n",
    "print(np.std(mse_test_loss))\n",
    "print(np.mean(like_test_loss))\n",
    "print(np.std(like_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14ba2f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06d0a93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3729d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1., 5., 7., 12.], [63., 15., 52., 172.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54737647",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22a7e24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "974729fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-07 09:56:17.748194: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-05-07 09:56:17.748356: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 4), dtype=float64, numpy=\n",
       "array([[ 12.,   7.,   1.,   5.],\n",
       "       [172.,  52.,  63.,  15.]])>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.transpose(tf.random.shuffle(tf.transpose(a, perm=[1, 0])), perm =[1, 0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88400bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_wrangler import feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45343f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "f  =feature_extractor.feature_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bf72fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=2.0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(2, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cf49dff",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Index out of range using input dim 2; input has only 2 dims [Op:StridedSlice] name: strided_slice/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ATP/data_wrangler/feature_extractor.py:55\u001b[0m, in \u001b[0;36mfeature_wrapper.permute\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x,  y\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# batch_s = tf.shape(x)[0]\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# repeated_x = tf.tile(x,  multiples=[num_permutation_repeats, 1, 1])\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# repeated_y = tf.tile(y,  multiples=[num_permutation_repeats, 1, 1])\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# Shuffle traget only. tf.random.shuffle only works on the first dimension so we need tf.transpose.\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     x_permuted \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat([tf\u001b[38;5;241m.\u001b[39mconcat([x[:,  :n_C, :], tf\u001b[38;5;241m.\u001b[39mtranspose(tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle(tf\u001b[38;5;241m.\u001b[39mtranspose(x[:, n_C:, :], perm\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m])), perm \u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m])], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_permutation_repeats)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)            \n\u001b[1;32m     56\u001b[0m     y_permuted \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat([tf\u001b[38;5;241m.\u001b[39mconcat([y[:,  :n_C, :], tf\u001b[38;5;241m.\u001b[39mtranspose(tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle(tf\u001b[38;5;241m.\u001b[39mtranspose(y[:, n_C:, :], perm\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m])), perm \u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m])], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_permutation_repeats)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# x_permuted = tf.concat([repeated_x[:,  :n_C, :],  x_target],  axis=1)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# y_permuted = tf.concat([repeated_y[:,  :n_C, :],  y_target],  axis=1)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ATP/data_wrangler/feature_extractor.py:55\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x,  y\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# batch_s = tf.shape(x)[0]\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# repeated_x = tf.tile(x,  multiples=[num_permutation_repeats, 1, 1])\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# repeated_y = tf.tile(y,  multiples=[num_permutation_repeats, 1, 1])\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# Shuffle traget only. tf.random.shuffle only works on the first dimension so we need tf.transpose.\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     x_permuted \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat([tf\u001b[38;5;241m.\u001b[39mconcat([\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43m:\u001b[49m\u001b[43mn_C\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m, tf\u001b[38;5;241m.\u001b[39mtranspose(tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle(tf\u001b[38;5;241m.\u001b[39mtranspose(x[:, n_C:, :], perm\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m])), perm \u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m])], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_permutation_repeats)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)            \n\u001b[1;32m     56\u001b[0m     y_permuted \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat([tf\u001b[38;5;241m.\u001b[39mconcat([y[:,  :n_C, :], tf\u001b[38;5;241m.\u001b[39mtranspose(tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle(tf\u001b[38;5;241m.\u001b[39mtranspose(y[:, n_C:, :], perm\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m])), perm \u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m])], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_permutation_repeats)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# x_permuted = tf.concat([repeated_x[:,  :n_C, :],  x_target],  axis=1)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# y_permuted = tf.concat([repeated_y[:,  :n_C, :],  y_target],  axis=1)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:7186\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7185\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7186\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Index out of range using input dim 2; input has only 2 dims [Op:StridedSlice] name: strided_slice/"
     ]
    }
   ],
   "source": [
    "f.permute([tf.constant(a, dtype=tf.float32), tf.constant(b, dtype=tf.float32), tf.constant(2, dtype=tf.int32), tf.constant(2, dtype=tf.int32), tf.constant(2, dtype=tf.int32)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094a6d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from comparison_models.gru import gru_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "665bcce2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gru_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m g1 \u001b[38;5;241m=\u001b[39m  \u001b[43mgru_pipeline\u001b[49m\u001b[38;5;241m.\u001b[39mgru_pipeline([\u001b[38;5;241m10\u001b[39m], \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gru_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "g1 =  gru_pipeline.gru_pipeline([10], 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0e00585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_wrangler import dataset_preparer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ab375b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "        x_train, y_train, x_val, y_val, x_test, y_test = dataset_preparer.weather_processor(path_to_weather_data=\"datasets/weather.csv\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb215cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_wrangler import batcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3504da1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_C = 10\n",
    "n_T = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a8cc1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "            idx_list = list(range(x_train.shape[0] - (n_C+n_T)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9dba53bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,_ = batcher.batcher(x_train,y_train,idx_list,window=n_C+n_T) ####### generalise for not just forecasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3717e265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 20, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dcc36b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = f.permute([x,y, 10, 10, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b99a4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
       "array([0.45411626, 0.46649173, 0.45411626, 0.39687976, 0.40770826,\n",
       "       0.45102248, 0.46958566, 0.50516504, 0.5330099 , 0.5438383 ,\n",
       "       0.6644992 , 0.5639485 , 0.5670424 , 0.6644992 , 0.56549543,\n",
       "       0.67223376, 0.6119035 , 0.68770313, 0.63974816, 0.5902464 ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0 , :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc6c3c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
       "array([0.45411626, 0.46649173, 0.45411626, 0.39687976, 0.40770826,\n",
       "       0.45102248, 0.46958566, 0.50516504, 0.5330099 , 0.5438383 ,\n",
       "       0.6119035 , 0.5902464 , 0.63974816, 0.56549543, 0.5639485 ,\n",
       "       0.5670424 , 0.68770313, 0.6644992 , 0.6644992 , 0.67223376],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[32, : , 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea2d22bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
       "array([[0.01103164],\n",
       "       [0.01750057],\n",
       "       [0.01374215],\n",
       "       [0.01171413],\n",
       "       [0.01860397],\n",
       "       [0.02116016],\n",
       "       [0.01885492],\n",
       "       [0.02241357],\n",
       "       [0.02199049],\n",
       "       [0.01917847]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, : ,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19806982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
       "array([[-0.2153583 ],\n",
       "       [-0.20701684],\n",
       "       [-0.2052439 ],\n",
       "       [-0.20571813],\n",
       "       [-0.21017832],\n",
       "       [-0.20565562],\n",
       "       [-0.21185149],\n",
       "       [-0.20508847],\n",
       "       [-0.20309193],\n",
       "       [-0.20570324]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[32, : ,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "334c6d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_C =10\n",
    "n_T =15\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "caed9a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "        x_mask = tf.linalg.band_part(tf.ones((n_T, n_C + n_T), tf.bool), -1, n_C)\n",
    "        x_mask_inv = (x_mask == False)\n",
    "        x_mask_float = tf.cast(x_mask_inv, \"float32\")*1000\n",
    "        x_mask_float_repeat = tf.repeat(x_mask_float[tf.newaxis, :], axis=0, repeats=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "29013095",
   "metadata": {},
   "outputs": [],
   "source": [
    "        x_mask = 1000  * (tf.constant(1, tf.int32) -  tf.linalg.band_part(tf.ones((n_T, n_C + n_T), tf.int32), -1, n_C))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b817fa78",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0., 1000., 1000., 1000., 1000., 1000., 1000., 1000.,\n",
       "        1000., 1000., 1000., 1000., 1000., 1000., 1000.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0., 1000., 1000., 1000., 1000., 1000., 1000.,\n",
       "        1000., 1000., 1000., 1000., 1000., 1000., 1000.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0., 1000., 1000., 1000., 1000., 1000.,\n",
       "        1000., 1000., 1000., 1000., 1000., 1000., 1000.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0., 1000., 1000., 1000., 1000.,\n",
       "        1000., 1000., 1000., 1000., 1000., 1000., 1000.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0., 1000., 1000., 1000.,\n",
       "        1000., 1000., 1000., 1000., 1000., 1000., 1000.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0., 1000., 1000.,\n",
       "        1000., 1000., 1000., 1000., 1000., 1000., 1000.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0., 1000.,\n",
       "        1000., 1000., 1000., 1000., 1000., 1000., 1000.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "        1000., 1000., 1000., 1000., 1000., 1000., 1000.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0., 1000., 1000., 1000., 1000., 1000., 1000.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0., 1000., 1000., 1000., 1000., 1000.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0., 1000., 1000., 1000., 1000.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0., 1000., 1000., 1000.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0., 1000., 1000.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0., 1000.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_mask_float.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12dbfbbe",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True, False, False, False, False, False, False, False,\n",
       "        False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True, False, False, False, False, False, False,\n",
       "        False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True, False, False, False, False, False,\n",
       "        False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True, False, False, False, False,\n",
       "        False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True, False, False, False,\n",
       "        False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "        False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_mask.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b927297",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 10, 20), dtype=float32, numpy=\n",
       "array([[[   0.,    0.,    0., ..., 1000., 1000., 1000.],\n",
       "        [   0.,    0.,    0., ..., 1000., 1000., 1000.],\n",
       "        [   0.,    0.,    0., ..., 1000., 1000., 1000.],\n",
       "        ...,\n",
       "        [   0.,    0.,    0., ...,    0., 1000., 1000.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0., 1000.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0.,    0.]],\n",
       "\n",
       "       [[   0.,    0.,    0., ..., 1000., 1000., 1000.],\n",
       "        [   0.,    0.,    0., ..., 1000., 1000., 1000.],\n",
       "        [   0.,    0.,    0., ..., 1000., 1000., 1000.],\n",
       "        ...,\n",
       "        [   0.,    0.,    0., ...,    0., 1000., 1000.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0., 1000.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0.,    0.]],\n",
       "\n",
       "       [[   0.,    0.,    0., ..., 1000., 1000., 1000.],\n",
       "        [   0.,    0.,    0., ..., 1000., 1000., 1000.],\n",
       "        [   0.,    0.,    0., ..., 1000., 1000., 1000.],\n",
       "        ...,\n",
       "        [   0.,    0.,    0., ...,    0., 1000., 1000.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0., 1000.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0.,    0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[   0.,    0.,    0., ..., 1000., 1000., 1000.],\n",
       "        [   0.,    0.,    0., ..., 1000., 1000., 1000.],\n",
       "        [   0.,    0.,    0., ..., 1000., 1000., 1000.],\n",
       "        ...,\n",
       "        [   0.,    0.,    0., ...,    0., 1000., 1000.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0., 1000.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0.,    0.]],\n",
       "\n",
       "       [[   0.,    0.,    0., ..., 1000., 1000., 1000.],\n",
       "        [   0.,    0.,    0., ..., 1000., 1000., 1000.],\n",
       "        [   0.,    0.,    0., ..., 1000., 1000., 1000.],\n",
       "        ...,\n",
       "        [   0.,    0.,    0., ...,    0., 1000., 1000.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0., 1000.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0.,    0.]],\n",
       "\n",
       "       [[   0.,    0.,    0., ..., 1000., 1000., 1000.],\n",
       "        [   0.,    0.,    0., ..., 1000., 1000., 1000.],\n",
       "        [   0.,    0.,    0., ..., 1000., 1000., 1000.],\n",
       "        ...,\n",
       "        [   0.,    0.,    0., ...,    0., 1000., 1000.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0., 1000.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0.,    0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_mask_float_repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94b80bec",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 20), dtype=float32, numpy=\n",
       "array([[   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0., 1000., 1000., 1000., 1000., 1000., 1000., 1000.,\n",
       "        1000., 1000.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0., 1000., 1000., 1000., 1000., 1000., 1000.,\n",
       "        1000., 1000.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0., 1000., 1000., 1000., 1000., 1000.,\n",
       "        1000., 1000.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0., 1000., 1000., 1000., 1000.,\n",
       "        1000., 1000.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0., 1000., 1000., 1000.,\n",
       "        1000., 1000.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0., 1000., 1000.,\n",
       "        1000., 1000.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0., 1000.,\n",
       "        1000., 1000.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "        1000., 1000.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0., 1000.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.]], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_mask_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37fc82da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 20), dtype=bool, numpy=\n",
       "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True, False, False, False, False, False, False, False,\n",
       "        False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True, False, False, False, False, False, False,\n",
       "        False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True, False, False, False, False, False,\n",
       "        False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True, False, False, False, False,\n",
       "        False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True, False, False, False,\n",
       "        False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "        False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True]])>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "650954b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    mask = np.tri(n_C + n_T, n_C + n_T, 0) - np.eye(n_C + n_T)\n",
    "    mask[:n_C, :n_C] = 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aff743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TNP mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37681e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 0., 0.],\n",
       "       [1., 1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tri(n_C + n_T, n_C + n_T, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9fce77ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.normal(size=(5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1cacd60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 0., 0.],\n",
       "       [1., 1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.linalg.band_part(tf.ones([n_C + n_T, n_C + n_T]), -1, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2ab11ae",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'tri'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mask_a \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtri\u001b[49m(n_C \u001b[38;5;241m+\u001b[39m n_T, n_C \u001b[38;5;241m+\u001b[39m n_T, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m tf\u001b[38;5;241m.\u001b[39meye(n_C \u001b[38;5;241m+\u001b[39m n_T)\n\u001b[1;32m      2\u001b[0m mask_a[:n_C, :n_C] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'tri'"
     ]
    }
   ],
   "source": [
    "    mask_a = tf.tri(n_C + n_T, n_C + n_T, 0) - tf.eye(n_C + n_T)\n",
    "    mask_a[:n_C, :n_C] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c92c76ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 5), dtype=float64, numpy=\n",
       "array([[1., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 0., 0.],\n",
       "       [1., 1., 1., 1., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 0., 0.],\n",
       "       [1., 1., 1., 1., 0.]])>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.concat([mask_a , mask_a], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ba6aa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "        context_part = tf.concat([tf.ones((n_C,n_C),tf.bool),tf.zeros((n_C,2*n_T),tf.bool)],\n",
    "                         axis=-1)\n",
    "        first_part = tf.linalg.band_part(tf.ones((n_T,n_C+2*n_T),tf.bool),-1,n_C)\n",
    "        second_part = tf.linalg.band_part(tf.ones((n_T,n_C+2*n_T),tf.bool),-1,n_C-1)\n",
    "        mask = tf.concat([context_part,first_part,second_part],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd4ca8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 8), dtype=bool, numpy=\n",
       "array([[ True,  True, False, False, False, False, False, False],\n",
       "       [ True,  True, False, False, False, False, False, False],\n",
       "       [ True,  True,  True, False, False, False, False, False],\n",
       "       [ True,  True,  True,  True, False, False, False, False],\n",
       "       [ True,  True,  True,  True,  True, False, False, False],\n",
       "       [ True,  True, False, False, False, False, False, False],\n",
       "       [ True,  True,  True, False, False, False, False, False],\n",
       "       [ True,  True,  True,  True, False, False, False, False]])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a117f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d9675a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290d76a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe0f013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "486debab",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ae1214ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "        y_temp = tf.squeeze(y[:, :n_C])\n",
    "        y_temp = tf.repeat(tf.expand_dims(y_temp,  axis=1),  axis=1,  repeats=n_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "22554914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 10, 10, 1])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f7eb798",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_C = 2\n",
    "n_T = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80d066d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "        context_part = tf.concat([tf.ones((n_C,n_C),tf.bool),tf.zeros((n_C,n_T),tf.bool)],axis=-1)\n",
    "        diagonal_mask = tf.linalg.band_part(tf.ones((n_C+n_T,n_C+n_T),tf.bool),-1,0)\n",
    "        lower_diagonal_mask = tf.linalg.set_diag(diagonal_mask,tf.zeros(diagonal_mask.shape[0:-1],tf.bool)) ### condense into one line?                                                                               \n",
    "        mask = tf.concat([context_part,lower_diagonal_mask[n_C:n_C+n_T,:n_C+n_T]],axis=0) # check no conflicts with init and check mask is correct shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85af570a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=bool, numpy=\n",
       "array([[ True,  True, False, False, False],\n",
       "       [ True,  True, False, False, False],\n",
       "       [ True,  True, False, False, False],\n",
       "       [ True,  True,  True, False, False],\n",
       "       [ True,  True,  True,  True, False]])>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4632741e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 0., 0.],\n",
       "       [1., 1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.linalg.band_part(tf.ones([n_C + n_T, n_C + n_T]), -1, 0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12d72d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       "array([[1., 1., 1., 1., 1.],\n",
       "       [0., 1., 1., 1., 1.],\n",
       "       [0., 0., 1., 1., 1.],\n",
       "       [0., 0., 0., 1., 1.],\n",
       "       [0., 0., 0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tf.linalg.band_part(tf.ones([n_C + n_T, n_C + n_T]), 0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5b4d2079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=array([1., 1., 1., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4394e49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
       "array([[1., 1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.concat([tf.linalg.band_part(tf.ones([n_C + n_T, n_C + n_T]), -1, 0)[n_C, :][tf.newaxis, :] for _ in range(n_C)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "504a0561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=bool, numpy=\n",
       "array([[ True,  True, False, False, False],\n",
       "       [ True,  True, False, False, False]])>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.repeat(tf.sequence_mask([n_C], n_C + n_T), 2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "663883a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=bool, numpy=\n",
       "array([[ True,  True,  True, False, False],\n",
       "       [ True,  True,  True,  True, False]])>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "tf.repeat(tf.sequence_mask([n_C], n_C + n_T), n_C+1, axis=0)\n",
    "tf.concat([tf.sequence_mask([n_C + i], n_C + n_T) for i in range(1, n_T)], axis=0)\n",
    "# diag = tf.cast(tf.linalg.band_part(tf.ones([n_C + n_T, n_C + n_T]), -1, 0)   - tf.eye(n_C + n_T), tf.bool)[n_C+1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e4e81045",
   "metadata": {},
   "outputs": [],
   "source": [
    "        context_part = tf.concat([tf.ones((n_C,n_C),tf.bool),tf.zeros((n_C,n_T),tf.bool)],axis=-1)\n",
    "        diagonal_mask = tf.linalg.band_part(tf.ones((n_C+n_T,n_C+n_T),tf.bool),-1,0)\n",
    "        lower_diagonal_mask = tf.linalg.set_diag(diagonal_mask,tf.zeros(diagonal_mask.shape[0:-1],tf.bool)) ### condense into one line?                                                                               \n",
    "        mask = tf.concat([context_part,lower_diagonal_mask[n_C:n_C+n_T,:n_C+n_T]],axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c4286e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=bool, numpy=\n",
       "array([[ True,  True, False, False, False],\n",
       "       [ True,  True, False, False, False]])>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c29040",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "939d6a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4efed2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 3 *x + np.random.normal(size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ead9b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class weighted_lr(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(weighted_lr, self).__init__()      \n",
    "        self.dense_a = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, x, query_x, training = True):\n",
    "        ŷ = self.dense_a(x)\n",
    "        weights = tf.concat([tf.math.exp(-1*tf.math.square(x[i] - query_x) / (2 * 0.05)) for i in range(x.shape[0])], axis=0)\n",
    "        return ŷ, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bc301239",
   "metadata": {},
   "outputs": [],
   "source": [
    "wlr = weighted_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ff289346",
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ, w = wlr(x[:15].reshape(-1, 1), x[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f796c674",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_graph():\n",
    "    \n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def train_step(wlr, optimizer, x, query_x, y, training=True):\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "            μ, weights = wlr(tf.cast(x, tf.float32), tf.cast(query_x, tf.float32)) \n",
    "            mse = tf.math.reduce_sum(weights * tf.math.square(tf.cast(y, tf.float32) - μ))\n",
    "        \n",
    "        gradients = tape.gradient(mse, wlr.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, wlr.trainable_variables))\n",
    "        return μ, mse\n",
    "\n",
    "    tf.keras.backend.set_floatx('float32')\n",
    "    return train_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "514fdd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_step = build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b75cf869",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(3e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "84b11b03",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 14:10:29.342235: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-05-10 14:10:29.342340: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(15, 1), dtype=float32, numpy=\n",
       " array([[3.2598412],\n",
       "        [3.1939857],\n",
       "        [3.1281304],\n",
       "        [3.062275 ],\n",
       "        [2.9964197],\n",
       "        [2.9305642],\n",
       "        [2.864709 ],\n",
       "        [2.7988534],\n",
       "        [2.7329984],\n",
       "        [2.6671426],\n",
       "        [2.6012874],\n",
       "        [2.535432 ],\n",
       "        [2.4695766],\n",
       "        [2.4037213],\n",
       "        [2.3378658]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=8392.003>)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_step(wlr, opt, x[:15].reshape(-1, 1), x[15], y[:15].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c07ee8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wlr.compile(x[:15].reshape(-1, 1), y[:15].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2a36be22",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Models passed to `fit` can only have `training` and the first argument in `call()` as positional arguments, found: ['query_x'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [122]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/keras/engine/training.py:2952\u001b[0m, in \u001b[0;36mModel._check_call_args\u001b[0;34m(self, method_name)\u001b[0m\n\u001b[1;32m   2950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(positional_args) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   2951\u001b[0m   extra_args \u001b[38;5;241m=\u001b[39m positional_args[\u001b[38;5;241m2\u001b[39m:]\n\u001b[0;32m-> 2952\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2953\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModels passed to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` can only have `training` \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2954\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand the first argument in `call()` as positional arguments, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2955\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfound: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextra_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Models passed to `fit` can only have `training` and the first argument in `call()` as positional arguments, found: ['query_x']."
     ]
    }
   ],
   "source": [
    "wlr.fit(x[:15].reshape(-1, 1), y[:15].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "703f1ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2744708"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(w.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e0ed5538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "7963bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_function(xi,x0,tau= .25): \n",
    "    return tf.math.exp( - (xi - x0)**2/(2*tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "f73b10e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xo = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "04f53b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xi = np.linspace(2, 10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "ae359003",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.sin(2*xi) + np.random.normal(0, 0.1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "f750c075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.        ,  2.08080808,  2.16161616,  2.24242424,  2.32323232,\n",
       "        2.4040404 ,  2.48484848,  2.56565657,  2.64646465,  2.72727273,\n",
       "        2.80808081,  2.88888889,  2.96969697,  3.05050505,  3.13131313,\n",
       "        3.21212121,  3.29292929,  3.37373737,  3.45454545,  3.53535354,\n",
       "        3.61616162,  3.6969697 ,  3.77777778,  3.85858586,  3.93939394,\n",
       "        4.02020202,  4.1010101 ,  4.18181818,  4.26262626,  4.34343434,\n",
       "        4.42424242,  4.50505051,  4.58585859,  4.66666667,  4.74747475,\n",
       "        4.82828283,  4.90909091,  4.98989899,  5.07070707,  5.15151515,\n",
       "        5.23232323,  5.31313131,  5.39393939,  5.47474747,  5.55555556,\n",
       "        5.63636364,  5.71717172,  5.7979798 ,  5.87878788,  5.95959596,\n",
       "        6.04040404,  6.12121212,  6.2020202 ,  6.28282828,  6.36363636,\n",
       "        6.44444444,  6.52525253,  6.60606061,  6.68686869,  6.76767677,\n",
       "        6.84848485,  6.92929293,  7.01010101,  7.09090909,  7.17171717,\n",
       "        7.25252525,  7.33333333,  7.41414141,  7.49494949,  7.57575758,\n",
       "        7.65656566,  7.73737374,  7.81818182,  7.8989899 ,  7.97979798,\n",
       "        8.06060606,  8.14141414,  8.22222222,  8.3030303 ,  8.38383838,\n",
       "        8.46464646,  8.54545455,  8.62626263,  8.70707071,  8.78787879,\n",
       "        8.86868687,  8.94949495,  9.03030303,  9.11111111,  9.19191919,\n",
       "        9.27272727,  9.35353535,  9.43434343,  9.51515152,  9.5959596 ,\n",
       "        9.67676768,  9.75757576,  9.83838384,  9.91919192, 10.        ])"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "8eccab0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1359562402.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [375]\u001b[0;36m\u001b[0m\n\u001b[0;31m    Y_temp = Y[]:ix+2]\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "yy = [] \n",
    "for ix, xo in enumerate(xi[2:]):\n",
    "    # for xi[2] xi_temp is xi[0], xi[1]\n",
    "    # for xi[3] xi_temp is xi[0], xi[1], xi[2]\n",
    "    xi_temp = xi[:ix+2]\n",
    "    n = xi_temp.shape[0]\n",
    "    Y_temp = Y[:ix+2]\n",
    "    Kxo = tf.cast(tf.linalg.diag(kernel_function(xi_temp, xo)), tf.float32)\n",
    "    Xxo = tf.cast(tf.concat([tf.ones([n, 1]), tf.reshape(tf.cast(xi_temp - xo, tf.float32), [n, 1])], axis=1), tf.float32)\n",
    "    β = tf.matmul(tf.matmul(tf.matmul(tf.linalg.inv(tf.matmul(tf.matmul(tf.transpose(Xxo), Kxo), Xxo)), tf.transpose(Xxo)), Kxo), Y_temp[:, tf.newaxis])\n",
    "    print(β[0].numpy())\n",
    "    print(β[1].numpy())\n",
    "    yy.append(β[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "c5678506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2ca25d460>]"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(yy)\n",
    "plt.plot(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "1b897535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "278ba15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "           \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f11bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-10.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
