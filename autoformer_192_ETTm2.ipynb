{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80b2c9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   --is_training 1 \\\n",
    "#   --root_path ./dataset/ETT-small/ \\\n",
    "#   --data_path ETTm2.csv \\\n",
    "#   --model_id ETTm2_96_288 \\\n",
    "#   --model Autoformer \\\n",
    "#   --data ETTm2 \\\n",
    "#   --features M \\\n",
    "#   --seq_len 96 \\\n",
    "#   --label_len 48 \\\n",
    "#   --pred_len 288 \\\n",
    "#   --e_layers 2 \\\n",
    "#   --d_layers 1 \\\n",
    "#   --factor 1 \\\n",
    "#   --enc_in 7 \\\n",
    "#   --dec_in 7 \\\n",
    "#   --c_out 7 \\\n",
    "#   --des 'Exp' \\\n",
    "#   --freq 't' \\\n",
    "#   --itr 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de150689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb7a4f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "elc = pd.read_csv('datasets/ETTm2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ec9c0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "621b1173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x127fe6550>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(elc.OT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b184f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d56bba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.tseries import offsets\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "\n",
    "\n",
    "class TimeFeature:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "class SecondOfMinute(TimeFeature):\n",
    "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.second / 59.0 - 0.5\n",
    "\n",
    "\n",
    "class MinuteOfHour(TimeFeature):\n",
    "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.minute / 59.0 - 0.5\n",
    "\n",
    "\n",
    "class HourOfDay(TimeFeature):\n",
    "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.hour / 23.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfWeek(TimeFeature):\n",
    "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.dayofweek / 6.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfMonth(TimeFeature):\n",
    "    \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.day - 1) / 30.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfYear(TimeFeature):\n",
    "    \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.dayofyear - 1) / 365.0 - 0.5\n",
    "\n",
    "\n",
    "class MonthOfYear(TimeFeature):\n",
    "    \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.month - 1) / 11.0 - 0.5\n",
    "\n",
    "\n",
    "class WeekOfYear(TimeFeature):\n",
    "    \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.isocalendar().week - 1) / 52.0 - 0.5\n",
    "\n",
    "\n",
    "def time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n",
    "    \"\"\"\n",
    "    Returns a list of time features that will be appropriate for the given frequency string.\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_str\n",
    "        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n",
    "    \"\"\"\n",
    "\n",
    "    features_by_offsets = {\n",
    "        offsets.YearEnd: [],\n",
    "        offsets.QuarterEnd: [MonthOfYear],\n",
    "        offsets.MonthEnd: [MonthOfYear],\n",
    "        offsets.Week: [DayOfMonth, WeekOfYear],\n",
    "        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Minute: [\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "        offsets.Second: [\n",
    "            SecondOfMinute,\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    offset = to_offset(freq_str)\n",
    "\n",
    "    for offset_type, feature_classes in features_by_offsets.items():\n",
    "        if isinstance(offset, offset_type):\n",
    "            return [cls() for cls in feature_classes]\n",
    "\n",
    "    supported_freq_msg = f\"\"\"\n",
    "    Unsupported frequency {freq_str}\n",
    "    The following frequencies are supported:\n",
    "        Y   - yearly\n",
    "            alias: A\n",
    "        M   - monthly\n",
    "        W   - weekly\n",
    "        D   - daily\n",
    "        B   - business days\n",
    "        H   - hourly\n",
    "        T   - minutely\n",
    "            alias: min\n",
    "        S   - secondly\n",
    "    \"\"\"\n",
    "    raise RuntimeError(supported_freq_msg)\n",
    "\n",
    "\n",
    "def time_features(dates, freq='h'):\n",
    "    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7dd9a093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c697d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Dataset_Custom(Dataset):\n",
    "    def __init__(self, flag='train', size=[192, 48, 96],\n",
    "                 features='S',\n",
    "                 target='OT', scale=True, timeenc=0, freq='h'):\n",
    "        # size [seq_len, label_len, pred_len]\n",
    "        # info\n",
    "        if size == None:\n",
    "            self.seq_len = 24 * 4 * 4\n",
    "            self.label_len = 24 * 4\n",
    "            self.pred_len = 24 * 4\n",
    "        else:\n",
    "            self.seq_len = size[0]\n",
    "            self.label_len = size[1]\n",
    "            self.pred_len = size[2]\n",
    "        # init\n",
    "        assert flag in ['train', 'test', 'val']\n",
    "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "        self.set_type = type_map[flag]\n",
    "\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.scale = scale\n",
    "        self.timeenc = timeenc\n",
    "        self.freq = freq\n",
    "\n",
    "        self.__read_data__()\n",
    "\n",
    "    def __read_data__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        df_raw = pd.read_csv('electricity.csv')\n",
    "\n",
    "        '''\n",
    "        df_raw.columns: ['date', ...(other features), target feature]\n",
    "        '''\n",
    "        cols = list(df_raw.columns)\n",
    "        cols.remove(self.target)\n",
    "        cols.remove('date')\n",
    "        df_raw = df_raw[['date'] + cols + [self.target]]\n",
    "        # print(cols)\n",
    "        num_train = int(len(df_raw) * 0.7)\n",
    "        num_test = int(len(df_raw) * 0.2)\n",
    "        num_vali = len(df_raw) - num_train - num_test\n",
    "        border1s = [0, num_train - self.seq_len, len(df_raw) - num_test - self.seq_len]\n",
    "        border2s = [num_train, num_train + num_vali, len(df_raw)]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "\n",
    "        if self.features == 'M' or self.features == 'MS':\n",
    "            cols_data = df_raw.columns[1:]\n",
    "            df_data = df_raw[cols_data]\n",
    "        elif self.features == 'S':\n",
    "            df_data = df_raw[[self.target]]\n",
    "\n",
    "        if self.scale:\n",
    "            train_data = df_data[border1s[0]:border2s[0]]\n",
    "            self.scaler.fit(train_data.values)\n",
    "            data = self.scaler.transform(df_data.values)\n",
    "        else:\n",
    "            data = df_data.values\n",
    "\n",
    "        df_stamp = df_raw[['date']][border1:border2]\n",
    "        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
    "        if self.timeenc == 0:\n",
    "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
    "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
    "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
    "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
    "            data_stamp = df_stamp.drop(['date'], 1).values\n",
    "        elif self.timeenc == 1:\n",
    "            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
    "            data_stamp = data_stamp.transpose(1, 0)\n",
    "\n",
    "        self.data_x = data[border1:border2]\n",
    "        self.data_y = data[border1:border2]\n",
    "        self.data_stamp = data_stamp\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "\n",
    "        seq_x = self.data_x[s_begin:s_end]\n",
    "        seq_y = self.data_y[r_begin:r_end]\n",
    "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
    "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
    "\n",
    "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac4fbc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97b05ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm\n",
    "import math\n",
    "\n",
    "def compared_version(ver1, ver2):\n",
    "    \"\"\"\n",
    "    :param ver1\n",
    "    :param ver2\n",
    "    :return: ver1< = >ver2 False/True\n",
    "    \"\"\"\n",
    "    list1 = str(ver1).split(\".\")\n",
    "    list2 = str(ver2).split(\".\")\n",
    "    \n",
    "    for i in range(len(list1)) if len(list1) < len(list2) else range(len(list2)):\n",
    "        if int(list1[i]) == int(list2[i]):\n",
    "            pass\n",
    "        elif int(list1[i]) < int(list2[i]):\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    if len(list1) == len(list2):\n",
    "        return True\n",
    "    elif len(list1) < len(list2):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        padding = 1 if compared_version(torch.__version__, '1.5.0') else 2\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
    "                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FixedEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(FixedEmbedding, self).__init__()\n",
    "\n",
    "        w = torch.zeros(c_in, d_model).float()\n",
    "        w.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, c_in).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        w[:, 0::2] = torch.sin(position * div_term)\n",
    "        w[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.emb = nn.Embedding(c_in, d_model)\n",
    "        self.emb.weight = nn.Parameter(w, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x).detach()\n",
    "\n",
    "\n",
    "class TemporalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='fixed', freq='h'):\n",
    "        super(TemporalEmbedding, self).__init__()\n",
    "\n",
    "        minute_size = 4\n",
    "        hour_size = 24\n",
    "        weekday_size = 7\n",
    "        day_size = 32\n",
    "        month_size = 13\n",
    "\n",
    "        Embed = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n",
    "        if freq == 't':\n",
    "            self.minute_embed = Embed(minute_size, d_model)\n",
    "        self.hour_embed = Embed(hour_size, d_model)\n",
    "        self.weekday_embed = Embed(weekday_size, d_model)\n",
    "        self.day_embed = Embed(day_size, d_model)\n",
    "        self.month_embed = Embed(month_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.long()\n",
    "\n",
    "        minute_x = self.minute_embed(x[:, :, 4]) if hasattr(self, 'minute_embed') else 0.\n",
    "        hour_x = self.hour_embed(x[:, :, 3])\n",
    "        weekday_x = self.weekday_embed(x[:, :, 2])\n",
    "        day_x = self.day_embed(x[:, :, 1])\n",
    "        month_x = self.month_embed(x[:, :, 0])\n",
    "\n",
    "        return hour_x + weekday_x + day_x + month_x + minute_x\n",
    "\n",
    "\n",
    "class TimeFeatureEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='timeF', freq='h'):\n",
    "        super(TimeFeatureEmbedding, self).__init__()\n",
    "\n",
    "        freq_map = {'h': 4, 't': 5, 's': 6, 'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n",
    "        d_inp = freq_map[freq]\n",
    "        self.embed = nn.Linear(d_inp, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)\n",
    "\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
    "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
    "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_mark):\n",
    "        x = self.value_embedding(x) + self.temporal_embedding(x_mark) + self.position_embedding(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class DataEmbedding_wo_pos(nn.Module):\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
    "        super(DataEmbedding_wo_pos, self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
    "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
    "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_mark):\n",
    "        x = self.value_embedding(x) + self.temporal_embedding(x_mark)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f194b78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class my_Layernorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Special designed layernorm for the seasonal part\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(my_Layernorm, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = self.layernorm(x)\n",
    "        bias = torch.mean(x_hat, dim=1).unsqueeze(1).repeat(1, x.shape[1], 1)\n",
    "        return x_hat - bias\n",
    "\n",
    "\n",
    "class moving_avg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer encoder layer with the progressive decomposition architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, attention, d_model, d_ff=None, moving_avg=25, dropout=0.1, activation=\"relu\"):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.attention = attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1, bias=False)\n",
    "        self.decomp1 = series_decomp(moving_avg)\n",
    "        self.decomp2 = series_decomp(moving_avg)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        new_x, attn = self.attention(\n",
    "            x, x, x,\n",
    "            attn_mask=attn_mask\n",
    "        )\n",
    "        x = x + self.dropout(new_x)\n",
    "        x, _ = self.decomp1(x)\n",
    "        y = x\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "        res, _ = self.decomp2(x + y)\n",
    "        return res, attn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attn_layers = nn.ModuleList(attn_layers)\n",
    "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
    "        self.norm = norm_layer\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        attns = []\n",
    "        if self.conv_layers is not None:\n",
    "            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                x = conv_layer(x)\n",
    "                attns.append(attn)\n",
    "            x, attn = self.attn_layers[-1](x)\n",
    "            attns.append(attn)\n",
    "        else:\n",
    "            for attn_layer in self.attn_layers:\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                attns.append(attn)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x, attns\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer decoder layer with the progressive decomposition architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, self_attention, cross_attention, d_model, c_out, d_ff=None,\n",
    "                 moving_avg=25, dropout=0.1, activation=\"relu\"):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.self_attention = self_attention\n",
    "        self.cross_attention = cross_attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1, bias=False)\n",
    "        self.decomp1 = series_decomp(moving_avg)\n",
    "        self.decomp2 = series_decomp(moving_avg)\n",
    "        self.decomp3 = series_decomp(moving_avg)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.projection = nn.Conv1d(in_channels=d_model, out_channels=c_out, kernel_size=3, stride=1, padding=1,\n",
    "                                    padding_mode='circular', bias=False)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None):\n",
    "        x = x + self.dropout(self.self_attention(\n",
    "            x, x, x,\n",
    "            attn_mask=x_mask\n",
    "        )[0])\n",
    "        x, trend1 = self.decomp1(x)\n",
    "        x = x + self.dropout(self.cross_attention(\n",
    "            x, cross, cross,\n",
    "            attn_mask=cross_mask\n",
    "        )[0])\n",
    "        x, trend2 = self.decomp2(x)\n",
    "        y = x\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "        x, trend3 = self.decomp3(x + y)\n",
    "\n",
    "        residual_trend = trend1 + trend2 + trend3\n",
    "        residual_trend = self.projection(residual_trend.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x, residual_trend\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, layers, norm_layer=None, projection=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.norm = norm_layer\n",
    "        self.projection = projection\n",
    "\n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None, trend=None):\n",
    "        for layer in self.layers:\n",
    "            x, residual_trend = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)\n",
    "            trend = trend + residual_trend\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        if self.projection is not None:\n",
    "            x = self.projection(x)\n",
    "        return x, trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28bb6185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from math import sqrt\n",
    "import os\n",
    "\n",
    "\n",
    "class AutoCorrelation(nn.Module):\n",
    "    \"\"\"\n",
    "    AutoCorrelation Mechanism with the following two phases:\n",
    "    (1) period-based dependencies discovery\n",
    "    (2) time delay aggregation\n",
    "    This block can replace the self-attention family mechanism seamlessly.\n",
    "    \"\"\"\n",
    "    def __init__(self, mask_flag=True, factor=1, scale=None, attention_dropout=0.1, output_attention=False):\n",
    "        super(AutoCorrelation, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def time_delay_agg_training(self, values, corr):\n",
    "        \"\"\"\n",
    "        SpeedUp version of Autocorrelation (a batch-normalization style design)\n",
    "        This is for the training phase.\n",
    "        \"\"\"\n",
    "        head = values.shape[1]\n",
    "        channel = values.shape[2]\n",
    "        length = values.shape[3]\n",
    "        # find top k\n",
    "        top_k = int(self.factor * math.log(length))\n",
    "        mean_value = torch.mean(torch.mean(corr, dim=1), dim=1)\n",
    "        index = torch.topk(torch.mean(mean_value, dim=0), top_k, dim=-1)[1]\n",
    "        weights = torch.stack([mean_value[:, index[i]] for i in range(top_k)], dim=-1)\n",
    "        # update corr\n",
    "        tmp_corr = torch.softmax(weights, dim=-1)\n",
    "        # aggregation\n",
    "        tmp_values = values\n",
    "        delays_agg = torch.zeros_like(values).float()\n",
    "        for i in range(top_k):\n",
    "            pattern = torch.roll(tmp_values, -int(index[i]), -1)\n",
    "            delays_agg = delays_agg + pattern * \\\n",
    "                         (tmp_corr[:, i].unsqueeze(1).unsqueeze(1).unsqueeze(1).repeat(1, head, channel, length))\n",
    "        return delays_agg\n",
    "\n",
    "    def time_delay_agg_inference(self, values, corr):\n",
    "        \"\"\"\n",
    "        SpeedUp version of Autocorrelation (a batch-normalization style design)\n",
    "        This is for the inference phase.\n",
    "        \"\"\"\n",
    "        batch = values.shape[0]\n",
    "        head = values.shape[1]\n",
    "        channel = values.shape[2]\n",
    "        length = values.shape[3]\n",
    "        # index init\n",
    "        init_index = torch.arange(length).unsqueeze(0).unsqueeze(0).unsqueeze(0)\\\n",
    "            .repeat(batch, head, channel, 1).to(values.device)\n",
    "        # find top k\n",
    "        top_k = int(self.factor * math.log(length))\n",
    "        mean_value = torch.mean(torch.mean(corr, dim=1), dim=1)\n",
    "        weights, delay = torch.topk(mean_value, top_k, dim=-1)\n",
    "        # update corr\n",
    "        tmp_corr = torch.softmax(weights, dim=-1)\n",
    "        # aggregation\n",
    "        tmp_values = values.repeat(1, 1, 1, 2)\n",
    "        delays_agg = torch.zeros_like(values).float()\n",
    "        for i in range(top_k):\n",
    "            tmp_delay = init_index + delay[:, i].unsqueeze(1).unsqueeze(1).unsqueeze(1).repeat(1, head, channel, length)\n",
    "            pattern = torch.gather(tmp_values, dim=-1, index=tmp_delay)\n",
    "            delays_agg = delays_agg + pattern * \\\n",
    "                         (tmp_corr[:, i].unsqueeze(1).unsqueeze(1).unsqueeze(1).repeat(1, head, channel, length))\n",
    "        return delays_agg\n",
    "\n",
    "    def time_delay_agg_full(self, values, corr):\n",
    "        \"\"\"\n",
    "        Standard version of Autocorrelation\n",
    "        \"\"\"\n",
    "        batch = values.shape[0]\n",
    "        head = values.shape[1]\n",
    "        channel = values.shape[2]\n",
    "        length = values.shape[3]\n",
    "        # index init\n",
    "        init_index = torch.arange(length).unsqueeze(0).unsqueeze(0).unsqueeze(0)\\\n",
    "            .repeat(batch, head, channel, 1).to(values.device)\n",
    "        # find top k\n",
    "        top_k = int(self.factor * math.log(length))\n",
    "        weights, delay = torch.topk(corr, top_k, dim=-1)\n",
    "        # update corr\n",
    "        tmp_corr = torch.softmax(weights, dim=-1)\n",
    "        # aggregation\n",
    "        tmp_values = values.repeat(1, 1, 1, 2)\n",
    "        delays_agg = torch.zeros_like(values).float()\n",
    "        for i in range(top_k):\n",
    "            tmp_delay = init_index + delay[..., i].unsqueeze(-1)\n",
    "            pattern = torch.gather(tmp_values, dim=-1, index=tmp_delay)\n",
    "            delays_agg = delays_agg + pattern * (tmp_corr[..., i].unsqueeze(-1))\n",
    "        return delays_agg\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, H, E = queries.shape\n",
    "        _, S, _, D = values.shape\n",
    "        if L > S:\n",
    "            zeros = torch.zeros_like(queries[:, :(L - S), :]).float()\n",
    "            values = torch.cat([values, zeros], dim=1)\n",
    "            keys = torch.cat([keys, zeros], dim=1)\n",
    "        else:\n",
    "            values = values[:, :L, :, :]\n",
    "            keys = keys[:, :L, :, :]\n",
    "\n",
    "        # period-based dependencies\n",
    "        q_fft = torch.fft.rfft(queries.permute(0, 2, 3, 1).contiguous(), dim=-1)\n",
    "        k_fft = torch.fft.rfft(keys.permute(0, 2, 3, 1).contiguous(), dim=-1)\n",
    "        res = q_fft * torch.conj(k_fft)\n",
    "        corr = torch.fft.irfft(res, dim=-1)\n",
    "\n",
    "        # time delay agg\n",
    "        if self.training:\n",
    "            V = self.time_delay_agg_training(values.permute(0, 2, 3, 1).contiguous(), corr).permute(0, 3, 1, 2)\n",
    "        else:\n",
    "            V = self.time_delay_agg_inference(values.permute(0, 2, 3, 1).contiguous(), corr).permute(0, 3, 1, 2)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return (V.contiguous(), corr.permute(0, 3, 1, 2))\n",
    "        else:\n",
    "            return (V.contiguous(), None)\n",
    "\n",
    "\n",
    "class AutoCorrelationLayer(nn.Module):\n",
    "    def __init__(self, correlation, d_model, n_heads, d_keys=None,\n",
    "                 d_values=None):\n",
    "        super(AutoCorrelationLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "        d_values = d_values or (d_model // n_heads)\n",
    "\n",
    "        self.inner_correlation = correlation\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "\n",
    "        out, attn = self.inner_correlation(\n",
    "            queries,\n",
    "            keys,\n",
    "            values,\n",
    "            attn_mask\n",
    "        )\n",
    "        out = out.view(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out), attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4685892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer is the first method to achieve the series-wise connection,\n",
    "    with inherent O(LlogL) complexity\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.seq_len = 288\n",
    "        self.label_len = 48\n",
    "        self.pred_len = 192\n",
    "        self.output_attention = True\n",
    "\n",
    "        # Decomp\n",
    "        kernel_size = 25\n",
    "        self.decomp = series_decomp(kernel_size)\n",
    "\n",
    "        # Embedding\n",
    "        # The series-wise connection inherently contains the sequential information.\n",
    "        # Thus, we can discard the position embedding of transformers.\n",
    "        self.enc_embedding = DataEmbedding_wo_pos(1, 20, 'timeF', 'h', 0.05)\n",
    "        self.dec_embedding = DataEmbedding_wo_pos(1, 20, 'timeF', 'h', 0.05)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AutoCorrelationLayer(\n",
    "                        AutoCorrelation(False, 3, attention_dropout=0.05,\n",
    "                                        output_attention=True),\n",
    "                        20, 2),\n",
    "                    20,\n",
    "                    512,\n",
    "                    moving_avg=25,\n",
    "                    dropout=0.05,\n",
    "                    activation='gelu'\n",
    "                ) for l in range(2)\n",
    "            ],\n",
    "            norm_layer=my_Layernorm(20)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            [\n",
    "                DecoderLayer(\n",
    "                    AutoCorrelationLayer(\n",
    "                        AutoCorrelation(True, 3, attention_dropout=0.05,\n",
    "                                        output_attention=False),\n",
    "                        20, 2),\n",
    "                    AutoCorrelationLayer(\n",
    "                        AutoCorrelation(False, 3, attention_dropout=0.05,\n",
    "                                        output_attention=False),\n",
    "                        20, 2),\n",
    "                    20,\n",
    "                    1,\n",
    "                    512,\n",
    "                    moving_avg=25,\n",
    "                    dropout=0.05,\n",
    "                    activation='gelu',\n",
    "                )\n",
    "                for l in range(1)\n",
    "            ],\n",
    "            norm_layer=my_Layernorm(20),\n",
    "            projection=nn.Linear(20, 1, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec,\n",
    "                enc_self_mask=None, dec_self_mask=None, dec_enc_mask=None):\n",
    "        # decomp init\n",
    "        mean = torch.mean(x_enc, dim=1).unsqueeze(1).repeat(1, self.pred_len, 1)\n",
    "        zeros = torch.zeros([x_dec.shape[0], self.pred_len, x_dec.shape[2]])\n",
    "        seasonal_init, trend_init = self.decomp(x_enc)\n",
    "        # decoder input\n",
    "        trend_init = torch.cat([trend_init[:, -self.label_len:, :], mean], dim=1)\n",
    "        seasonal_init = torch.cat([seasonal_init[:, -self.label_len:, :], zeros], dim=1)\n",
    "        # enc\n",
    "        enc_out = self.enc_embedding(x_enc.float(), x_mark_enc.float())\n",
    "        enc_out, attns = self.encoder(enc_out, attn_mask=enc_self_mask)\n",
    "        # dec\n",
    "        dec_out = self.dec_embedding(seasonal_init.float(), x_mark_dec[:, self.label_len:, :].float())\n",
    "        seasonal_part, trend_part = self.decoder(dec_out, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask,\n",
    "                                                 trend=trend_init)\n",
    "        # final\n",
    "        dec_out = trend_part + seasonal_part\n",
    "\n",
    "        if self.output_attention:\n",
    "            return dec_out[:, -self.pred_len:, :], attns\n",
    "        else:\n",
    "            return dec_out[:, -self.pred_len:, :]  # [B, L, D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df063ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>HUFL</th>\n",
       "      <th>HULL</th>\n",
       "      <th>MUFL</th>\n",
       "      <th>MULL</th>\n",
       "      <th>LUFL</th>\n",
       "      <th>LULL</th>\n",
       "      <th>OT</th>\n",
       "      <th>d</th>\n",
       "      <th>Year</th>\n",
       "      <th>month</th>\n",
       "      <th>hr</th>\n",
       "      <th>min</th>\n",
       "      <th>day</th>\n",
       "      <th>weekday</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01 00:00:00</td>\n",
       "      <td>41.130001</td>\n",
       "      <td>12.481</td>\n",
       "      <td>36.535999</td>\n",
       "      <td>9.355</td>\n",
       "      <td>4.424</td>\n",
       "      <td>1.311</td>\n",
       "      <td>38.661999</td>\n",
       "      <td>2016-07-01 00:00:00</td>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2016.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-07-01 00:15:00</td>\n",
       "      <td>39.622002</td>\n",
       "      <td>11.309</td>\n",
       "      <td>35.543999</td>\n",
       "      <td>8.551</td>\n",
       "      <td>3.209</td>\n",
       "      <td>1.258</td>\n",
       "      <td>38.223000</td>\n",
       "      <td>2016-07-01 00:15:00</td>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2016.500028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-07-01 00:30:00</td>\n",
       "      <td>38.868000</td>\n",
       "      <td>10.555</td>\n",
       "      <td>34.365002</td>\n",
       "      <td>7.586</td>\n",
       "      <td>4.435</td>\n",
       "      <td>1.258</td>\n",
       "      <td>37.344002</td>\n",
       "      <td>2016-07-01 00:30:00</td>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2016.500056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-07-01 00:45:00</td>\n",
       "      <td>35.518002</td>\n",
       "      <td>9.214</td>\n",
       "      <td>32.569000</td>\n",
       "      <td>8.712</td>\n",
       "      <td>4.435</td>\n",
       "      <td>1.215</td>\n",
       "      <td>37.124001</td>\n",
       "      <td>2016-07-01 00:45:00</td>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2016.500084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-07-01 01:00:00</td>\n",
       "      <td>37.528000</td>\n",
       "      <td>10.136</td>\n",
       "      <td>33.936001</td>\n",
       "      <td>7.532</td>\n",
       "      <td>4.435</td>\n",
       "      <td>1.215</td>\n",
       "      <td>37.124001</td>\n",
       "      <td>2016-07-01 01:00:00</td>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2016.500112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69675</th>\n",
       "      <td>2018-06-26 18:45:00</td>\n",
       "      <td>42.722000</td>\n",
       "      <td>12.230</td>\n",
       "      <td>54.014000</td>\n",
       "      <td>12.652</td>\n",
       "      <td>-11.525</td>\n",
       "      <td>-1.418</td>\n",
       "      <td>45.986500</td>\n",
       "      <td>2018-06-26 18:45:00</td>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>45</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>2018.485971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69676</th>\n",
       "      <td>2018-06-26 19:00:00</td>\n",
       "      <td>38.868000</td>\n",
       "      <td>10.052</td>\n",
       "      <td>49.859001</td>\n",
       "      <td>10.669</td>\n",
       "      <td>-11.525</td>\n",
       "      <td>-1.418</td>\n",
       "      <td>45.986500</td>\n",
       "      <td>2018-06-26 19:00:00</td>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>2018.485999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69677</th>\n",
       "      <td>2018-06-26 19:15:00</td>\n",
       "      <td>39.622002</td>\n",
       "      <td>11.057</td>\n",
       "      <td>50.448002</td>\n",
       "      <td>11.795</td>\n",
       "      <td>-10.299</td>\n",
       "      <td>-1.418</td>\n",
       "      <td>45.546501</td>\n",
       "      <td>2018-06-26 19:15:00</td>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>2018.486027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69678</th>\n",
       "      <td>2018-06-26 19:30:00</td>\n",
       "      <td>40.459999</td>\n",
       "      <td>11.392</td>\n",
       "      <td>51.841999</td>\n",
       "      <td>11.929</td>\n",
       "      <td>-11.536</td>\n",
       "      <td>-1.418</td>\n",
       "      <td>45.546501</td>\n",
       "      <td>2018-06-26 19:30:00</td>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>30</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>2018.486055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69679</th>\n",
       "      <td>2018-06-26 19:45:00</td>\n",
       "      <td>43.223999</td>\n",
       "      <td>12.146</td>\n",
       "      <td>54.737000</td>\n",
       "      <td>12.679</td>\n",
       "      <td>-11.536</td>\n",
       "      <td>-1.418</td>\n",
       "      <td>45.327000</td>\n",
       "      <td>2018-06-26 19:45:00</td>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>45</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>2018.486083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69680 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date       HUFL    HULL       MUFL    MULL    LUFL  \\\n",
       "0      2016-07-01 00:00:00  41.130001  12.481  36.535999   9.355   4.424   \n",
       "1      2016-07-01 00:15:00  39.622002  11.309  35.543999   8.551   3.209   \n",
       "2      2016-07-01 00:30:00  38.868000  10.555  34.365002   7.586   4.435   \n",
       "3      2016-07-01 00:45:00  35.518002   9.214  32.569000   8.712   4.435   \n",
       "4      2016-07-01 01:00:00  37.528000  10.136  33.936001   7.532   4.435   \n",
       "...                    ...        ...     ...        ...     ...     ...   \n",
       "69675  2018-06-26 18:45:00  42.722000  12.230  54.014000  12.652 -11.525   \n",
       "69676  2018-06-26 19:00:00  38.868000  10.052  49.859001  10.669 -11.525   \n",
       "69677  2018-06-26 19:15:00  39.622002  11.057  50.448002  11.795 -10.299   \n",
       "69678  2018-06-26 19:30:00  40.459999  11.392  51.841999  11.929 -11.536   \n",
       "69679  2018-06-26 19:45:00  43.223999  12.146  54.737000  12.679 -11.536   \n",
       "\n",
       "        LULL         OT                   d  Year  month  hr  min  day  \\\n",
       "0      1.311  38.661999 2016-07-01 00:00:00  2016      7   0    0    1   \n",
       "1      1.258  38.223000 2016-07-01 00:15:00  2016      7   0   15    1   \n",
       "2      1.258  37.344002 2016-07-01 00:30:00  2016      7   0   30    1   \n",
       "3      1.215  37.124001 2016-07-01 00:45:00  2016      7   0   45    1   \n",
       "4      1.215  37.124001 2016-07-01 01:00:00  2016      7   1    0    1   \n",
       "...      ...        ...                 ...   ...    ...  ..  ...  ...   \n",
       "69675 -1.418  45.986500 2018-06-26 18:45:00  2018      6  18   45   26   \n",
       "69676 -1.418  45.986500 2018-06-26 19:00:00  2018      6  19    0   26   \n",
       "69677 -1.418  45.546501 2018-06-26 19:15:00  2018      6  19   15   26   \n",
       "69678 -1.418  45.546501 2018-06-26 19:30:00  2018      6  19   30   26   \n",
       "69679 -1.418  45.327000 2018-06-26 19:45:00  2018      6  19   45   26   \n",
       "\n",
       "       weekday            t  \n",
       "0            4  2016.500000  \n",
       "1            4  2016.500028  \n",
       "2            4  2016.500056  \n",
       "3            4  2016.500084  \n",
       "4            4  2016.500112  \n",
       "...        ...          ...  \n",
       "69675        1  2018.485971  \n",
       "69676        1  2018.485999  \n",
       "69677        1  2018.486027  \n",
       "69678        1  2018.486055  \n",
       "69679        1  2018.486083  \n",
       "\n",
       "[69680 rows x 16 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c249b6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ED = elc\n",
    "\n",
    "    \n",
    "ED['d'] = pd.to_datetime(ED['date'])\n",
    "ED['Year'] = ED['date'].apply(lambda x: int(x[:10][:4]))\n",
    "ED['month'] = ED['date'].apply(lambda x: int(x[:10][5:7]))\n",
    "ED['hr'] = ED['date'].apply(lambda x: int(x[11:][:2]))\n",
    "ED['min'] = ED['date'].apply(lambda x: int(x[11:][3:5]))\n",
    "ED['day'] = ED.d.apply(lambda row: row.day, 1)\n",
    "ED['weekday'] = ED.d.apply(lambda row: row.weekday(), 1)\n",
    "\n",
    "ED['t'] = ED['Year'] + ((ED['month'] - 1)/ 12) + ((ED['day'] - 1)/ (12 *31) + ED['hr']/(24*31*12) + ED['min']/(60*24*31*12))\n",
    "t = np.array(ED['t'])\n",
    "ED_13 = (ED.loc[ED.t >= 2013, :])\n",
    "\n",
    "client_w_time = (ED.loc[ED.t >= 2013, ['Year', 'month', 'day', 'weekday', 'hr','min', 'OT']])\n",
    "time_feat = ED.loc[ED.t >= 2013, ['Year', 'month', 'day', 'weekday', 'hr','min']]\n",
    "\n",
    "\n",
    "t_13 = ED_13.index\n",
    "train_last_ix = int(len(t_13) *0.8)\n",
    "target = np.array(ED_13['OT'])\n",
    "arr_kwh = target \n",
    "μ_arr =  np.mean(arr_kwh[:train_last_ix]).reshape(-1, 1)\n",
    "σ_arr =  np.std(arr_kwh[:train_last_ix]).reshape(-1, 1)\n",
    "arr_kwh_a = (arr_kwh - μ_arr)/σ_arr\n",
    "time_features = client_w_time.iloc[:, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79bc3bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batcher(time_features, target, batch_s = 32, training=True):\n",
    "    enc_l = 96\n",
    "    dec_l = 192\n",
    "    win_l = enc_l + dec_l\n",
    "    train_last_ix = int((target.shape[0]) *0.7)\n",
    "    val_last_ix = int((target.shape[0]) *0.8)\n",
    "\n",
    "    if training:\n",
    "        target = target[:train_last_ix]\n",
    "        time_features = time_features[:train_last_ix, :]\n",
    "    else: \n",
    "        target = target[train_last_ix:val_last_ix]\n",
    "        time_features = time_features[train_last_ix:val_last_ix, :]\n",
    "    # print(np.arange(win_l, len(target) - win_l))\n",
    "    idx = np.random.choice(np.arange(win_l, len(target) - win_l), batch_s)\n",
    "    tar = np.array([target[i:i+win_l] for i in idx])\n",
    "    t = np.array([np.array(time_features)[i:i+win_l, :] for i in idx])\n",
    "    \n",
    "    x_enc = tar[:, :enc_l].reshape(batch_s, enc_l, 1); \n",
    "\n",
    "    x_dec = tar[:, :win_l].reshape(batch_s, win_l, 1)\n",
    "    y_tar = x_dec[:, -192:, :].copy()\n",
    "    x_dec[:, -192:, :] = 0\n",
    "    \n",
    "    x_mark_enc = t[:, :enc_l, :]; \n",
    "    x_mark_dec = t[:, :win_l, :]\n",
    "    \n",
    "    return x_enc, x_dec, x_mark_enc, x_mark_dec, y_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f1a080ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4be34285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 288, 1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "829d3b0b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3072x5 and 4x20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x_mark\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_tar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_y_mark\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec, enc_self_mask, dec_self_mask, dec_enc_mask)\u001b[0m\n\u001b[1;32m     73\u001b[0m seasonal_init \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([seasonal_init[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_len:, :], zeros], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# enc\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m enc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menc_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mark_enc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m enc_out, attns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(enc_out, attn_mask\u001b[38;5;241m=\u001b[39menc_self_mask)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# dec\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36mDataEmbedding_wo_pos.forward\u001b[0;34m(self, x, x_mark)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, x_mark):\n\u001b[0;32m--> 156\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_embedding(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemporal_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_mark\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36mTimeFeatureEmbedding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3072x5 and 4x20)"
     ]
    }
   ],
   "source": [
    "        outputs = model(torch.tensor(batch_x), torch.tensor(batch_x_mark), torch.tensor(y_tar), torch.tensor(batch_y_mark))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "384ce4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69680, 5)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc44f96f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3072x5 and 4x20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m model_optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# decoder input\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x_mark\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_tar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_y_mark\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m192\u001b[39m:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     23\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y_tar[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m192\u001b[39m:, \u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec, enc_self_mask, dec_self_mask, dec_enc_mask)\u001b[0m\n\u001b[1;32m     73\u001b[0m seasonal_init \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([seasonal_init[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_len:, :], zeros], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# enc\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m enc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menc_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mark_enc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m enc_out, attns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(enc_out, attn_mask\u001b[38;5;241m=\u001b[39menc_self_mask)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# dec\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36mDataEmbedding_wo_pos.forward\u001b[0;34m(self, x, x_mark)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, x_mark):\n\u001b[0;32m--> 156\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_embedding(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemporal_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_mark\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36mTimeFeatureEmbedding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3072x5 and 4x20)"
     ]
    }
   ],
   "source": [
    "best_models = []\n",
    "for j in range(5):\n",
    "    mini = 5000\n",
    "    model = Model()\n",
    "    model_optim = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    criterion = nn.MSELoss()\n",
    "    iter_count = 0\n",
    "    epoch_time = time.time()\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for i in range(40000):\n",
    "        model.train()\n",
    "\n",
    "\n",
    "\n",
    "        batch_x, batch_y, batch_x_mark, batch_y_mark, y_tar = batcher(np.array(time_features), (arr_kwh_a)[0]) \n",
    "        iter_count += 1\n",
    "        model_optim.zero_grad()\n",
    "        # decoder input\n",
    "        outputs = model(torch.tensor(batch_x), torch.tensor(batch_x_mark), torch.tensor(y_tar), torch.tensor(batch_y_mark))\n",
    "\n",
    "        outputs = outputs[0][:, -192:, 0]\n",
    "        batch_y = torch.tensor(y_tar[:, -192:, 0])\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        model_optim.step()\n",
    "        if (i + 1) % 500 == 0:\n",
    "            # evaluate model:\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_x, batch_y, batch_x_mark, batch_y_mark, y_tar = batcher(np.array(time_features), (arr_kwh_a)[0], batch_s=200, training=False) \n",
    "                out_data = model(torch.tensor(batch_x), torch.tensor(batch_x_mark), torch.tensor(y_tar), torch.tensor(batch_y_mark))\n",
    "                out_data = out_data[0][:, -192:, 0]\n",
    "                batch_y = torch.tensor(y_tar[:, -192:, 0])\n",
    "                loss_v = criterion(out_data, batch_y)\n",
    "                val_loss.append(loss_v.item())\n",
    "                if loss_v < mini:\n",
    "                    mini = loss_v\n",
    "                    torch.save(model.state_dict(), '192_out_best-model-parameters_{}.pt'.format(j)) # official recommended\n",
    "\n",
    "            print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, i + 1, loss.item()))\n",
    "            print(\"\\titers: {0}, epoch: {1} | loss val: {2:.7f}\".format(i + 1, i + 1, loss_v.item()))\n",
    "            print(\"iter: {} cost time: {}\".format(iter_count + 1, time.time() - epoch_time))\n",
    "            epoch_time = time.time()\n",
    "    \n",
    "    best_models.append(np.min(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9afebdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_size=0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c28572c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274404"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2d2ce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = np.load('merged_192_out_96_input.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f76d6172",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = np.load('x_all_test_set_electricity_192_out.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e65e91af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "96+192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e2f777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_delete = np.array([33, 46, 54, 71, 82, 105, 166, 188, 200, 201, 218, 229, 269, 289, 302, 330, 429, 450, 491, 492, 495, 498, 519, 521, 549, 561, 595, 602, 613, 619, 622, 663, 678, 708, 725, 745, 817, 866, 868, 870, 890, 911, 916, 925, 926, 945, 974,\n",
    "891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 912, 913, 914, 915, 917, 918, 919, 920, 921, 922, 923, 924, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a707645f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = np.delete(merged, rows_to_delete, axis=0)\n",
    "x_all = np.delete(x_all, rows_to_delete, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24d3372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_l = 96\n",
    "win_l = 288\n",
    "batch_s = 850"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8f0a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    x_enc = merged[:, :enc_l, :].reshape(batch_s, enc_l, 1); \n",
    "\n",
    "    x_dec = merged[:, :win_l, :].reshape(batch_s, win_l, 1)\n",
    "    y_tar = merged[:, -192:, :].copy()\n",
    "    x_dec[:, -192:, :] = 0\n",
    "    \n",
    "    x_mark_enc = x_all[:, :enc_l, :]; \n",
    "    x_mark_dec = x_all[:, :win_l, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e8f58b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "import math as ma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5d901ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_lik(y, ŷ, log_σ, ϵ=0.001):\n",
    "    pi = torch.tensor(ma.pi)\n",
    "    mse_per_point = torch.square(torch.subtract(y, ŷ))\n",
    "    lik_per_point = (-1 / 2) * (torch.divide(mse_per_point, torch.square(torch.exp(log_σ)) + ϵ)) - 1*torch.log(torch.exp(log_σ) + ϵ)- (1/2)*torch.log(2*pi)\n",
    "    return lik_per_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d550ae22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11715958668776452\n",
      "0.0014377348423265112\n",
      "-0.3454768017976543\n",
      "0.006190499667256998\n"
     ]
    }
   ],
   "source": [
    "mse_test_loss = []\n",
    "like_test_loss = []\n",
    "for i in range(5):\n",
    "    model = Model() # we do not specify pretrained=True, i.e. do not load default weights\n",
    "    model.load_state_dict(torch.load('192_out_best-model-parameters_{}.pt'.format(i)))\n",
    "    model.eval()\n",
    "    outputs_te = model(torch.tensor(x_enc), torch.tensor(x_mark_enc), torch.tensor(y_tar), torch.tensor(x_mark_dec))\n",
    "    batch_y = torch.tensor(y_tar)\n",
    "    loss = criterion(outputs_te[0], batch_y)\n",
    "    mse_test_loss.append(loss.detach().numpy())\n",
    "    log_σ = np.log(np.sqrt(mse_test_loss[-1]))\n",
    "    lik_pp = seq_lik(batch_y, outputs_te[0], torch.tensor(log_σ))\n",
    "    like_test_loss.append(np.mean(lik_pp.detach().numpy()))\n",
    "print(np.mean(mse_test_loss))\n",
    "print(np.std(mse_test_loss))\n",
    "print(np.mean(like_test_loss))\n",
    "print(np.std(like_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70d243d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-10.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
