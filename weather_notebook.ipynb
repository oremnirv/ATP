{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import atp_graph, losses\n",
    "from Tutorials.helper import batcher\n",
    "from data import synthetic_data_gen, feature_extractor\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import IPython\n",
    "import time\n",
    "import os\n",
    "\n",
    "from model import atp_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import dataset_preparer\n",
    "\n",
    "#dataset - [weather,traffic,...]\n",
    "\n",
    "#hyperparams - num_heads,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_processor = dataset_preparer.weather(path_to_weather_data=\"datasets/weather.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x_train, y_train, x_val, y_val, x_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mweather_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/hpcdata/users/rapart57/conda-envs/updated_tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/data/hpcdata/users/rapart57/conda-envs/updated_tf/lib/python3.10/site-packages/keras/utils/layer_utils.py:805\u001b[0m, in \u001b[0;36mCallFunctionSpec.split_out_first_arg\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    803\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    804\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 805\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arg_names\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    806\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(kwargs)\n\u001b[1;32m    807\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arg_names[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_val, y_val, x_test, y_test = weather_processor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"weights/forecasting/weather\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(442)\n",
    "opt = tf.keras.optimizers.Adam(3e-4)\n",
    "run=3; heads=2\n",
    "atp_model = atp_pipeline.atp_pipeline(num_heads=num_heads)\n",
    "EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_l = 96+192;\n",
    "num_batches = 500\n",
    "batch_s=20\n",
    "mini = 50000\n",
    "n_C_te =5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.InitializationOnlyStatus at 0x7fb3ec1213c0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 1\n",
    "tr_step = atp_graph.build_graph()\n",
    "\n",
    "name_comp = 'run_' + str(run)\n",
    "logdir = save_dir + '/logs/' + name_comp\n",
    "folder = save_dir + '/ckpt/check_' + name_comp\n",
    "\n",
    "if not os.path.exists(folder): os.mkdir(folder)\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=opt, net=atp_model)\n",
    "manager = tf.train.CheckpointManager(ckpt, folder, max_to_keep=1)\n",
    "ckpt.restore(manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch 8 validation lik pp: 0.6718450784683228\n",
      "Time taken for 1 epoch: 28.91314935684204 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    print(\"epoch: \",+epoch)\n",
    "    start = time.time()\n",
    "    for batch_n in range(10):\n",
    "         \n",
    "        x,y = batcher(training_data_scaled[:,:1],training_data_scaled[:,-2:])\n",
    "        n_C = int(np.random.choice(np.arange(2, 20), 1))\n",
    "        n_T = 288 - n_C\n",
    "        y_tr = y\n",
    "        t_tr = x\n",
    "        _,_, nll_pp, msex = tr_step(atp_model, opt, t_tr,y_tr,n_C,n_T, training=True)\n",
    "\n",
    "        if ((batch_n % 2 == 0)):\n",
    "            \n",
    "            n_C = 10\n",
    "            n_T = 200 - n_C\n",
    "            t_te,y_te = batcher(val_data_scaled[:,:1], val_data_scaled[:,-2:],batch_s = 100)\n",
    "            μ, log_σ = atp_model([t_te, y_te, 10, 200, False])\n",
    "            _,_,_, nll_pp_te, msex_te = losses.nll(y_te[:, n_C:n_C+n_T], μ[:, n_C:], log_σ[:, n_C:])\n",
    "            \n",
    "        if nll_pp_te < mini:\n",
    "            mini = nll_pp_te\n",
    "            IPython.display.clear_output(wait=True)\n",
    "            print(\"epoch {} batch {} validation lik pp: {}\".format(epoch, batch_n, nll_pp)) \n",
    "            manager.save()\n",
    "            step += 1\n",
    "            ckpt.step.assign_add(1)\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 16:59:28.722885: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 16:59:33.194323: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 16:59:38.228749: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 16:59:42.366600: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 16:59:48.822430: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 16:59:53.695908: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:00:04.466689: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:00:09.120972: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:00:15.745944: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 17:00:23.658776: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:00:30.318846: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:00:40.900534: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:00:47.548930: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 17:01:14.395057: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:01:18.993984: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:01:28.693503: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:01:39.207029: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:01:45.224822: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:01:51.007337: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:01:56.811759: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:02:04.584574: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 17:02:15.950181: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:02:23.240941: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:02:37.292670: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:02:49.628143: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 17:03:09.371770: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:03:15.845362: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:03:22.843914: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:03:29.248879: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:03:34.032447: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:03:40.983235: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:03:47.358786: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:03:58.376806: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:04:06.648857: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:04:16.593402: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 17:04:28.134780: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:04:40.436230: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:04:48.145242: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 17:05:17.694155: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:05:29.173872: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:05:35.320544: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:05:41.366539: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:05:51.312592: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:06:04.222915: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 17:06:16.750343: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:06:28.825705: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:06:39.023297: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:06:45.185655: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 17:07:09.548425: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:07:14.081493: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:07:21.297863: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:07:33.094102: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:07:43.536016: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:07:53.878573: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:08:05.723638: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 17:08:20.023947: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:08:34.902275: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:08:46.535644: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:09:00.301088: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 17:09:15.021889: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# nll_list = []; mse_list = []\n",
    "# num_epochs = 2;num_batches = 10; test_batch_s = 100\n",
    "# num_runs = 5 ### used to have a std over likelhood results\n",
    "\n",
    "# for run in range(5):\n",
    "#     sum_mse_tot = 0; sum_nll_tot = 0\n",
    "#     tf.random.set_seed(run)\n",
    "    \n",
    "#     mini = 50000\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         print(\"epoch: \",+epoch)\n",
    "#         start = time.time()\n",
    "#         for batch_n in range(num_batches):\n",
    "#             x,y = batcher(training_data_scaled[:,:1],training_data_scaled[:,-2:])\n",
    "#             n_C = int(np.random.choice(np.arange(2, 20), 1))\n",
    "#             n_T = 288 - n_C\n",
    "#             _,_, nll_pp, msex = tr_step(atp_model, opt, x,y,n_C,n_T, training=True)\n",
    "            \n",
    "    \n",
    "    \n",
    "#     for _ in range(test_data_scaled.shape[0] // test_batch_s):\n",
    "#         n_C = 10\n",
    "#         n_T = 200 - n_C\n",
    "#         t_te,y_te = batcher(test_data_scaled[:,:1], test_data_scaled[:,-2:], batch_s = test_batch_s)\n",
    "#         μ, log_σ = atp_model([t_te,y_te,10,200, False])\n",
    "#         _, sum_mse, sum_nll, _, _ = losses.nll(y_te[:, n_C:n_C+n_T], μ[:, n_C:], log_σ[:, n_C:])\n",
    "#         sum_nll_tot += sum_nll # each sequence is length 190 times 100 seq. per batch \n",
    "#         sum_mse_tot += sum_mse\n",
    "        \n",
    "#     nllx =  sum_nll_tot / (n_T * test_batch_s * (test_data_scaled.shape[0] // test_batch_s))\n",
    "#     msex =  sum_mse_tot / (n_T * test_batch_s * (test_data_scaled.shape[0] // test_batch_s))\n",
    "    \n",
    "        \n",
    "#     nll_list.append(nllx.numpy())\n",
    "#     mse_list.append(msex.numpy())\n",
    "# np.save(save_dir + '/nll_list.npy', nll_list)    \n",
    "# np.save(save_dir + '/mse_list.npy', mse_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Max\n",
      "epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 20:21:57.231179: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-04-30 20:21:57.231429: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2023-04-30 20:21:59.990918: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-04-30 20:21:59.991420: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 20:22:02.881981: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 20:22:05.996027: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 20:22:09.376753: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 20:22:12.245687: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 7 calls to <function build_graph.<locals>.train_step at 0x2b4f059d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 20:22:15.302552: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 8 calls to <function build_graph.<locals>.train_step at 0x2b4f059d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 20:22:18.807729: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 20:22:22.502387: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 20:22:26.338168: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 20:22:30.944002: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 20:22:35.810029: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 20:22:39.532378: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-30 20:22:44.754942: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "testing.testing_multiple_times([training_data_scaled, test_data_scaled], save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_repeats):\n",
    "    \n",
    "    # instantiate model\n",
    "    \n",
    "    \n",
    "    \n",
    "    # train model for num_epochs\n",
    "        # if loss on val set < mini, save model weights\n",
    "    \n",
    "    # reload model from model weights\n",
    "    \n",
    "    # evaluate model on test set and append to test set array\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "        training_data_scaled, test_data_scaled = inputs\n",
    "        #instantiation\n",
    "        \n",
    "        step = 1\n",
    "        run=run_num; \n",
    "        tf.random.set_seed(run)\n",
    "        opt = tf.keras.optimizers.Adam(3e-4)\n",
    "        atp_model = atp_pipeline.atp_pipeline( num_heads=num_heads, projection_shape_for_head=projection_shape_for_head, output_shape=output_shape, rate=rate, permutation_repeats=permutation_repeats,\n",
    "        bound_std=bound_std, num_layers=num_layers, enc_dim=enc_dim,  xmin=xmin, xmax=xmax)\n",
    "        tr_step = atp_graph.build_graph()\n",
    "        name_comp = 'run_' + str(run)\n",
    "        folder = save_dir + '/ckpt/check_' + name_comp\n",
    "        if not os.path.exists(folder): os.mkdir(folder)\n",
    "        ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=opt, net=atp_model)\n",
    "        manager = tf.train.CheckpointManager(ckpt, folder, max_to_keep=1)\n",
    "        ckpt.restore(manager.latest_checkpoint) \n",
    "        sum_mse_tot = 0; sum_nll_tot = 0\n",
    "        mini = =50000\n",
    "        \n",
    "        #training\n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"epoch: \",+epoch)\n",
    "            for _ in range(num_batches):\n",
    "                x,y = batcher(training_data_scaled[:,:1],training_data_scaled[:,-2:])\n",
    "                n_C = int(np.random.choice(np.arange(2, 20), 1))\n",
    "                n_T = 288 - n_C\n",
    "                _,_, _, _ = tr_step(atp_model, opt, x,y,n_C,n_T, training=True)\n",
    "                \n",
    "                #saving \n",
    "                \n",
    "                if nll_pp_te < mini:\n",
    "                    mini = nll_pp_te\n",
    "                    manager.save()\n",
    "                    step += 1\n",
    "                    ckpt.step.assign_add(1)\n",
    "        \n",
    "        ckpt.restore(manager.latest_checkpoint) \n",
    "\n",
    "\n",
    "        for _ in range(test_data_scaled.shape[0] // test_batch_s):\n",
    "            n_C = 10\n",
    "            n_T = 200 - n_C\n",
    "            t_te,y_te = batcher(test_data_scaled[:,:1], test_data_scaled[:,-2:], batch_s = test_batch_s)\n",
    "            μ, log_σ = atp_model([t_te,y_te,10,200, False])\n",
    "            _, sum_mse, sum_nll, _, _ = losses.nll(y_te[:, n_C:n_C+n_T], μ[:, n_C:], log_σ[:, n_C:])\n",
    "            sum_nll_tot += sum_nll # each sequence is length 190 times 100 seq. per batch \n",
    "            sum_mse_tot += sum_mse\n",
    "            \n",
    "        nllx =  sum_nll_tot / (n_T * test_batch_s * (test_data_scaled.shape[0] // test_batch_s))\n",
    "        msex =  sum_mse_tot / (n_T * test_batch_s * (test_data_scaled.shape[0] // test_batch_s))\n",
    "        \n",
    "            \n",
    "        nll_list.append(nllx.numpy())\n",
    "        mse_list.append(msex.numpy())\n",
    "    np.save(save_dir + '/nll_list.npy', nll_list)    \n",
    "    np.save(save_dir + '/mse_list.npy', mse_list)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "\n",
    "#electricity\n",
    "\n",
    "# make batcher as a function of n_C and n_T - window\n",
    "\n",
    "## other NP experiments\n",
    "\n",
    "# test on n_C and n_T which are randomly selected at each point - outside batcher \n",
    "\n",
    "# don't repeat testing points  \n",
    "\n",
    "# for NP experiments, we need to batch differently. for forecasting 100000 x1 ---> 32 x 288 x 1.\n",
    "# for np, we will have 10000 x 288 x 1 -> 32 x 288 x 1\n",
    "\n",
    "#also divide mse by n_T each batch for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set this up for training and saving best models adn then loading it and calculating loglik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batcher(t, y, idx_list,batch_s = 32, window = 288,cut_sequence = True):\n",
    "    '''\n",
    "    cutting one long array to sequences of length 'window'.\n",
    "    'batch_s' must be ≤ full array - window length\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    if len(idx_list) < 1:\n",
    "        print(\"warning- you didn't loop over the correct range\")\n",
    "        \n",
    "    \n",
    "    batch_s = min(batch_s, y.shape[0]-window)    \n",
    "    idx = np.random.choice(len(idx_list), batch_s, replace = False)\n",
    "\n",
    "    y = np.array([np.array(y)[idx_list[i]:idx_list[i]+window] for i in idx])\n",
    "    t = np.array([np.array(t)[idx_list[i]:idx_list[i]+window] for i in idx])\n",
    "    for i in sorted(idx, reverse=True): del idx_list[i]\n",
    "    return t, y,idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tr_ix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.9\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[43mY\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])), batch_s)\n\u001b[1;32m      2\u001b[0m y_tr \u001b[38;5;241m=\u001b[39m Y[tr_ix, :, :]\n\u001b[1;32m      3\u001b[0m t_tr \u001b[38;5;241m=\u001b[39m t[tr_ix, :]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y' is not defined"
     ]
    }
   ],
   "source": [
    "y_tr = Y[tr_ix, :, :]\n",
    "t_tr = t[tr_ix, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36887, 1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batcher(t, y, idx_list, batch_s = 32, window = 288):\n",
    "    '''\n",
    "    cutting one long array to sequences of length 'window'.\n",
    "    'batch_s' must be ≤ full array - window length\n",
    "\n",
    "    input to forecast: (None, 1, 1) for t,y.\n",
    "    input to NP tasks: (None, seq_len, 1) for t,y.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    if len(idx_list) < 1:\n",
    "        print(\"warning- you didn't loop over the correct range\")\n",
    "        \n",
    "    \n",
    "    batch_s = min(batch_s, y.shape[0]-window)    \n",
    "    idx = np.random.choice(len(idx_list), batch_s, replace = False)\n",
    "\n",
    "    y = np.array([np.array(y)[idx_list[i]:idx_list[i]+window, :, :] for i in idx])\n",
    "    t = np.array([np.array(t)[idx_list[i]:idx_list[i]+window, :, :] for i in idx])\n",
    "    for i in sorted(idx, reverse=True): del idx_list[i]\n",
    "        \n",
    "    t = t.squeeze()\n",
    "    y = y.squeeze()\n",
    "    \n",
    "    if len(t.shape) == 2:\n",
    "        t = t[:,:,np.newaxis]\n",
    "        y = y[:,:,np.newaxis]\n",
    "        \n",
    "    return t,y, idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_list=list(range(y_test.shape[0]-288))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = batcher(x_test[:,:,np.newaxis],y_test[:,:,np.newaxis],idx_list=idx_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 288)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 288)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dummy = np.random.normal(size=(100,300,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dummy = np.random.normal(size=(100,300,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_list=list(range(y_dummy.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 300, 1)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dummy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = batcher(x_dummy,y_dummy,idx_list=idx_list,window=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 300)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import atp_graph, losses\n",
    "from data import synthetic_data_gen, feature_extractor\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from model import atp_pipeline\n",
    "from data import dataset_preparer\n",
    "import argparse\n",
    "from Tutorials.helper import batcher\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_val, y_val, x_test, y_test = dataset_preparer.weather_processor(path_to_weather_data=\"datasets/weather.csv\") \n",
    "save_dir = \"weights/forecasting/weather\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_C = 100\n",
    "n_T = 100\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(3e-4)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36887, 1, 1)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 200, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "idx_list = list(range(x_train.shape[0] - (n_C+n_T)))\n",
    "x, y, _ = batcher(x_train, y_train, idx_list, window=n_C+n_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 200, 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5269, 1, 1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_C+n_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5269, 1, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5269, 1, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batcher_x(t, y, idx_list, batch_s = 32, window = 288):\n",
    "    '''\n",
    "    cutting one long array to sequences of length 'window'.\n",
    "    'batch_s' must be ≤ full array - window length\n",
    "\n",
    "    input to forecast: (None, 1, 1) for t,y.\n",
    "    input to NP tasks: (None, seq_len, 1) for t,y. window = 1.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    if len(idx_list) < 1:\n",
    "        print(\"warning- you didn't loop over the correct range\")\n",
    "        \n",
    "    \n",
    "    batch_s = min(batch_s, y.shape[0]-window)    \n",
    "    idx = np.random.choice(len(idx_list), batch_s, replace = False)\n",
    "\n",
    "    print(idx)\n",
    "    y = np.array([np.array(y)[idx_list[i]:idx_list[i]+window, :, :] for i in idx])\n",
    "    t = np.array([np.array(t)[idx_list[i]:idx_list[i]+window, :, :] for i in idx])\n",
    "    print(t.shape)\n",
    "    for i in sorted(idx, reverse=True): del idx_list[i]\n",
    "        \n",
    "    t = t.squeeze()\n",
    "    y = y.squeeze()\n",
    "    \n",
    "    if len(t.shape) == 2:\n",
    "        t = t[:,:,np.newaxis]\n",
    "        y = y[:,:,np.newaxis]\n",
    "        \n",
    "    return t,y, idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5269, 1, 1)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 587 2204 2195  154 4901  523 4876 2513  727 4387 4751 3855 2354 3288\n",
      " 1611 2136 1968 2405 2622 3900  374 4298 4600  664 2177 1851 2616 3719\n",
      "  816 2745 1284 1076 2460 1846 1373 4937 3889 1970 2538 3753 3091 2870\n",
      " 2013 2889  146  250  397 2574 3027 2313  277 1701  404 4847 3644 1877\n",
      "  529 3008 2963 3482 2067 5027  377 1685 3658  569 4153 1739 2988 2392\n",
      " 3636 1092 1451 2203  352 4850 1884  381  347 2257 4784 4446  517 3235\n",
      " 4429 4746 2572  580 4641 3556 3365  990  596 3190 4061 3341 4536 1184\n",
      " 1025 2681]\n",
      "(100, 200, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "idx_list_v = list(range(x_val.shape[0] - (n_C+n_T)))\n",
    "t_te,y_te,_ = batcher_x(x_val, y_val, idx_list_v, batch_s = 100, window=(n_C+n_T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 200, 1)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**why is batcher failing for cell 27 i.e. the validation batching?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 18:45:51.039065: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-05-02 18:45:51.041177: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Max\n",
      "(32, 200, 1, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/omernivron/Documents/ATP/model/atp_graph.py\", line 18, in train_step  *\n        optimizer.apply_gradients(zip(gradients, atp_model.trainable_variables))\n    File \"/Users/omernivron/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 639, in apply_gradients  **\n        self._create_all_weights(var_list)\n    File \"/Users/omernivron/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 825, in _create_all_weights\n        self._create_slots(var_list)\n    File \"/Users/omernivron/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/keras/optimizer_v2/adam.py\", line 117, in _create_slots\n        self.add_slot(var, 'm')\n    File \"/Users/omernivron/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 918, in add_slot\n        self._restore_slot_variable(\n    File \"/Users/omernivron/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 1342, in _restore_slot_variable\n        checkpoint_position.restore(slot_variable)\n\n    ValueError: Shapes (1, 16) and (2, 16) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m idx_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(x_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     17\u001b[0m x,y,_ \u001b[38;5;241m=\u001b[39m batcher(x_train,y_train,idx_list,window\u001b[38;5;241m=\u001b[39mn_C\u001b[38;5;241m+\u001b[39mn_T) \u001b[38;5;66;03m####### generalise for not weather\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m _,_, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtr_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43matp_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_C\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_T\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     21\u001b[0m     idx_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(x_val\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m   1148\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/omernivron/Documents/ATP/model/atp_graph.py\", line 18, in train_step  *\n        optimizer.apply_gradients(zip(gradients, atp_model.trainable_variables))\n    File \"/Users/omernivron/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 639, in apply_gradients  **\n        self._create_all_weights(var_list)\n    File \"/Users/omernivron/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 825, in _create_all_weights\n        self._create_slots(var_list)\n    File \"/Users/omernivron/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/keras/optimizer_v2/adam.py\", line 117, in _create_slots\n        self.add_slot(var, 'm')\n    File \"/Users/omernivron/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 918, in add_slot\n        self._restore_slot_variable(\n    File \"/Users/omernivron/miniforge3/envs/tf_m1_n/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 1342, in _restore_slot_variable\n        checkpoint_position.restore(slot_variable)\n\n    ValueError: Shapes (1, 16) and (2, 16) are incompatible\n"
     ]
    }
   ],
   "source": [
    "step = 1\n",
    "run= 50 + 0\n",
    "tf.random.set_seed(run)\n",
    "atp_model = atp_pipeline.instantiate_atp(\"weather\")\n",
    "tr_step = atp_graph.build_graph()\n",
    "name_comp = 'run_' + str(run)\n",
    "folder = save_dir + '/ckpt/check_' + name_comp\n",
    "if not os.path.exists(folder): os.mkdir(folder)\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=opt, net=atp_model)\n",
    "manager = tf.train.CheckpointManager(ckpt, folder, max_to_keep=1)\n",
    "ckpt.restore(manager.latest_checkpoint) \n",
    "sum_mse_tot = 0; sum_nll_tot = 0\n",
    "mini = 50000\n",
    "\n",
    "for i in range(2):\n",
    "    idx_list = list(range(x_train.shape[0]))\n",
    "    x,y,_ = batcher(x_train,y_train,idx_list,window=n_C+n_T) ####### generalise for not weather\n",
    "    _,_, _, _ = tr_step(atp_model, opt, x,y,n_C,n_T, training=True)\n",
    "\n",
    "    if i % 1 == 0:\n",
    "        idx_list = list(range(x_val.shape[0]))\n",
    "        t_te,y_te,_ = batcher(x_val,y_val,idx_list,batch_s = 100,window=n_C+n_T)\n",
    "        μ, log_σ = atp_model([t_te, y_te, n_C, n_T, False])\n",
    "        _,_,_, nll_pp_te, msex_te = losses.nll(y_te[:, n_C:n_C+n_T], μ, log_σ)\n",
    "\n",
    "        if nll_pp_te < mini:\n",
    "            mini = nll_pp_te\n",
    "            manager.save()\n",
    "            step += 1\n",
    "            ckpt.step.assign_add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36887, 1, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5269, 1, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "        t_te,y_te,_ = batcher(x_train,y_train,idx_list,batch_s = 32,window=n_C+n_T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 200, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36887, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
